{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importeren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "import warnings\n",
    "import pyodbc\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to: c:\\Users\\veyse\\source\\DEDS_portfolio\\data\\raw\\AdventureWorks.mdb\n",
      "Database file found at c:\\Users\\veyse\\source\\DEDS_portfolio\\data\\raw\\AdventureWorks.mdb\n",
      "Connected to Access database successfully!\n",
      "<pyodbc.Connection object at 0x000002E4D3EF6E80>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyodbc\n",
    "\n",
    "# Define the path to your Access database using absolute path\n",
    "# This ensures there's no ambiguity in the path resolution\n",
    "access_db_path = os.path.abspath('../../data/raw/AdventureWorks.mdb')\n",
    "print(f\"Attempting to connect to: {access_db_path}\")\n",
    "\n",
    "# Verify the file exists\n",
    "if not os.path.exists(access_db_path):\n",
    "    print(f\"Error: Database file not found at {access_db_path}\")\n",
    "    # Try to list files in the directory to help troubleshoot\n",
    "    dir_path = os.path.dirname(access_db_path)\n",
    "    if os.path.exists(dir_path):\n",
    "        print(f\"Files in {dir_path}:\")\n",
    "        for file in os.listdir(dir_path):\n",
    "            print(f\"  - {file}\")\n",
    "    else:\n",
    "        print(f\"Directory {dir_path} does not exist\")\n",
    "else:\n",
    "    print(f\"Database file found at {access_db_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Build the connection string for Access\n",
    "        access_conn_str = (\n",
    "            r\"Driver={Microsoft Access Driver (*.mdb, *.accdb)};\"\n",
    "            r\"DBQ=\" + access_db_path + \";\"\n",
    "        )\n",
    "        \n",
    "        # Attempt to establish the connection\n",
    "        access_conn = pyodbc.connect(access_conn_str)\n",
    "        print(\"Connected to Access database successfully!\")\n",
    "        print(access_conn)\n",
    "        \n",
    "        # Don't forget to close the connection when done\n",
    "        access_conn.close()\n",
    "        \n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        \n",
    "        # Check if driver is available\n",
    "        print(\"\\nChecking available ODBC drivers:\")\n",
    "        for driver in pyodbc.drivers():\n",
    "            print(f\"  - {driver}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlite3.Connection object at 0x000001319481BE20>\n",
      "Connected to Access database!\n",
      "<pyodbc.Connection object at 0x00000131B4F8B1C0>\n",
      "Connected to SQL Server database!\n",
      "<pyodbc.Connection object at 0x00000131B4F8B020>\n"
     ]
    }
   ],
   "source": [
    "connection_aenc = sql.connect('../../data/raw/aenc.sqlite')\n",
    "print(connection_aenc)\n",
    "\n",
    "# Define the path to your Access database\n",
    "access_db_path = '../../data/raw/AdventureWorks.mdb'\n",
    "\n",
    "# Build the connection string for Access\n",
    "access_conn_str = (\n",
    "    r\"Driver={Microsoft Access Driver (*.mdb, *.accdb)};\"\n",
    "    r\"Dbq=\" + access_db_path + \";\"\n",
    ")\n",
    "\n",
    "# Establish the connection\n",
    "access_conn = pyodbc.connect(access_conn_str)\n",
    "print(\"Connected to Access database!\")\n",
    "print(access_conn)\n",
    "\n",
    "\n",
    "# Build the connection string for SQL Server\n",
    "sql_server_conn_str = (\n",
    "    r\"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    r\"SERVER=LAPTOP-HLMO0COA\\SQLEXPRESS;\"\n",
    "    r\"DATABASE=northwind;\"\n",
    "    r\"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "# Establish the connection\n",
    "sql_server_conn = pyodbc.connect(sql_server_conn_str)\n",
    "print(\"Connected to SQL Server database!\")\n",
    "print(sql_server_conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legen van Tabbelen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Disabling constraints for Bonus...\n",
      "  Disabling constraints for Customer...\n",
      "  Disabling constraints for EmployeeTerritories...\n",
      "  Disabling constraints for OrderDetails...\n",
      "  Disabling constraints for Orders...\n",
      "  Disabling constraints for purchaseOrderDetail...\n",
      "  Disabling constraints for purchaseOrderHeader...\n",
      "  Disabling constraints for Sales_Order...\n",
      "  Disabling constraints for sales_order_item...\n",
      "  Disabling constraints for Sales_OrderDetail...\n",
      "  Disabling constraints for Sales_OrderHeader...\n",
      "  Disabling constraints for Sales_Store...\n",
      "  Disabling constraints for Territories...\n",
      "All constraints disabled\n",
      " Legen van tabel: Address\n",
      " Legen van tabel: billOfMaterials\n",
      " Legen van tabel: Bonus\n",
      " Legen van tabel: Categories\n",
      " Legen van tabel: Customer\n",
      " Legen van tabel: Department\n",
      " Legen van tabel: Employee\n",
      " Legen van tabel: EmployeeTerritories\n",
      " Legen van tabel: OrderDetails\n",
      " Legen van tabel: Orders\n",
      " Legen van tabel: person\n",
      " Legen van tabel: Product\n",
      " Legen van tabel: productCategory\n",
      " Legen van tabel: purchaseOrderDetail\n",
      " Legen van tabel: purchaseOrderHeader\n",
      " Legen van tabel: purchasing_vendor\n",
      " Legen van tabel: Region\n",
      " Legen van tabel: resources_department\n",
      " Legen van tabel: Sales_Order\n",
      " Legen van tabel: sales_order_item\n",
      " Legen van tabel: Sales_OrderDetail\n",
      " Legen van tabel: Sales_OrderHeader\n",
      " Legen van tabel: Sales_Store\n",
      " Legen van tabel: Sales_territory\n",
      " Legen van tabel: Shippers\n",
      " Legen van tabel: State\n",
      " Legen van tabel: Suppliers\n",
      " Legen van tabel: Territories\n",
      "  Re-enabling constraints for Bonus...\n",
      "  Re-enabling constraints for Customer...\n",
      "  Re-enabling constraints for EmployeeTerritories...\n",
      "  Re-enabling constraints for OrderDetails...\n",
      "  Re-enabling constraints for Orders...\n",
      "  Re-enabling constraints for purchaseOrderDetail...\n",
      "  Re-enabling constraints for purchaseOrderHeader...\n",
      "  Re-enabling constraints for Sales_Order...\n",
      "  Re-enabling constraints for sales_order_item...\n",
      "  Re-enabling constraints for Sales_OrderDetail...\n",
      "  Re-enabling constraints for Sales_OrderHeader...\n",
      "  Re-enabling constraints for Sales_Store...\n",
      "  Re-enabling constraints for Territories...\n",
      "All constraints re-enabled successfully\n",
      " Alle tabellen in de database 'sdm' zijn geleegd.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    'SERVER=LAPTOP-HLMO0COA\\\\SQLEXPRESS;' \n",
    "    'DATABASE=DWH_project;' \n",
    "    'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get all tables with constraints\n",
    "tables_query = \"\"\"\n",
    "SELECT DISTINCT OBJECT_NAME(parent_object_id) AS TableName\n",
    "FROM sys.foreign_keys\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(tables_query)\n",
    "    tables_with_constraints = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    # Disable constraints for each table individually\n",
    "    for table in tables_with_constraints:\n",
    "        print(f\"  Disabling constraints for {table}...\")\n",
    "        cursor.execute(f\"ALTER TABLE [{table}] NOCHECK CONSTRAINT ALL\")\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"All constraints disabled\")\n",
    "except Exception as e:\n",
    "    print(f\"Error disabling constraints: {str(e)}\")\n",
    "    # Continue anyway - we'll still try to import data\n",
    "\n",
    "# Get all tables\n",
    "cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n",
    "all_tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Empty each table\n",
    "for table_name in all_tables:\n",
    "    print(f\" Legen van tabel: {table_name}\")\n",
    "    try:\n",
    "        cursor.execute(f\"DELETE FROM [{table_name}];\")\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting from {table_name}: {str(e)}\")\n",
    "\n",
    "# Re-enable constraints\n",
    "try:\n",
    "    # Re-enable constraints for each table individually\n",
    "    for table in tables_with_constraints:\n",
    "        print(f\"  Re-enabling constraints for {table}...\")\n",
    "        cursor.execute(f\"ALTER TABLE [{table}] WITH CHECK CHECK CONSTRAINT ALL\")\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"All constraints re-enabled successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error re-enabling constraints: {str(e)}\")\n",
    "\n",
    "print(\" Alle tabellen in de database 'sdm' zijn geleegd.\")\n",
    "\n",
    "# Close connections at the very end\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqlite importeren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SQL Server database\n",
      "Found 30 tables in SQL Server database\n",
      "Disabling constraints...\n",
      "Processing SQLite database: aenc.sqlite\n",
      "Found tables: ['Department', 'Product', 'State', 'Bonus', 'Customer', 'Employee', 'Sales_Order', 'Sales_Order_Item']\n",
      "Importing table: Department\n",
      "Successfully imported table: Department\n",
      "Importing table: Product\n",
      "Successfully imported table: Product\n",
      "Importing table: State\n",
      "Successfully imported table: State\n",
      "Importing table: Bonus\n",
      "Successfully imported table: Bonus\n",
      "Importing table: Customer\n",
      "Successfully imported table: Customer\n",
      "Importing table: Employee\n",
      "Successfully imported table: Employee\n",
      "Importing table: Sales_Order\n",
      "Successfully imported table: Sales_Order\n",
      "Importing table: Sales_Order_Item\n",
      "Successfully imported table: Sales_Order_Item\n",
      "Re-enabling constraints...\n",
      "Database import completed successfully\n",
      "Import completed successfully.\n"
     ]
    }
   ],
   "source": [
    "def convert_numpy_type(val):\n",
    "    if pd.isnull(val):\n",
    "        return None\n",
    "    elif hasattr(val, 'item'):  # Handles numpy types (int64, float64, etc.)\n",
    "        return val.item()\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "# Helper function to handle type conversions\n",
    "def convert_value_for_column(value, column_name, column_type, table_name):\n",
    "    \"\"\"Converts a value to the appropriate type for the target column.\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert based on target column type\n",
    "    try:\n",
    "        if 'INT' in column_type.upper():\n",
    "            # For INT columns\n",
    "            if isinstance(value, str):\n",
    "                # Try to convert string to int\n",
    "                try:\n",
    "                    return int(value)\n",
    "                except ValueError:\n",
    "                    print(f\"Cannot convert '{value}' to INT for {table_name}.{column_name}, using default\")\n",
    "                    return 0\n",
    "            return int(value) if value is not None else None\n",
    "        \n",
    "        elif 'REAL' in column_type.upper() or 'FLOAT' in column_type.upper() or 'DECIMAL' in column_type.upper():\n",
    "            # For floating-point columns\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    return float(value)\n",
    "                except ValueError:\n",
    "                    print(f\"Cannot convert '{value}' to REAL for {table_name}.{column_name}, using default\")\n",
    "                    return 0.0\n",
    "            return float(value) if value is not None else None\n",
    "        \n",
    "        elif 'TEXT' in column_type.upper() or 'VARCHAR' in column_type.upper() or 'CHAR' in column_type.upper():\n",
    "            # For text columns\n",
    "            return str(value) if value is not None else None\n",
    "        \n",
    "        elif 'DATE' in column_type.upper():\n",
    "            # For date columns\n",
    "            return value\n",
    "        \n",
    "        elif 'BIT' in column_type.upper():\n",
    "            # For boolean columns\n",
    "            if isinstance(value, bool):\n",
    "                return 1 if value else 0\n",
    "            elif isinstance(value, (int, float)):\n",
    "                return 1 if value > 0 else 0\n",
    "            elif isinstance(value, str):\n",
    "                return 1 if value.lower() in ('true', 'yes', 'y', '1') else 0\n",
    "            return 0\n",
    "            \n",
    "        else:\n",
    "            # Default handling for other types\n",
    "            return value\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting value '{value}' for {table_name}.{column_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get column types from a table\n",
    "def get_column_types(cursor, table_name):\n",
    "    \"\"\"Gets the data types of columns in a table.\"\"\"\n",
    "    column_types = {}\n",
    "    cursor.execute(f\"SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\")\n",
    "    for row in cursor.fetchall():\n",
    "        column_types[row[0]] = row[1]\n",
    "    return column_types\n",
    "\n",
    "# Function to check if a column has a NOT NULL constraint\n",
    "def get_nullable_columns(cursor, table_name):\n",
    "    \"\"\"Gets information about which columns are nullable.\"\"\"\n",
    "    nullable_info = {}\n",
    "    cursor.execute(f\"SELECT COLUMN_NAME, IS_NULLABLE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\")\n",
    "    for row in cursor.fetchall():\n",
    "        nullable_info[row[0]] = row[1] == 'YES'\n",
    "    return nullable_info\n",
    "\n",
    "def import_data():\n",
    "    try:\n",
    "        # Connect to SQL Server\n",
    "        conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-HLMO0COA\\\\SQLEXPRESS;DATABASE=SDM_Project;Trusted_Connection=yes;')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(\"Connected to SQL Server database\")\n",
    "        \n",
    "        # Get list of tables in SQL Server\n",
    "        cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n",
    "        sql_tables = [row[0] for row in cursor.fetchall()]\n",
    "        print(f\"Found {len(sql_tables)} tables in SQL Server database\")\n",
    "        \n",
    "        # Disable constraints\n",
    "        print(\"Disabling constraints...\")\n",
    "        cursor.execute(\"SELECT DISTINCT OBJECT_NAME(parent_object_id) FROM sys.foreign_keys\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"ALTER TABLE [{table}] NOCHECK CONSTRAINT ALL\")\n",
    "        conn.commit()\n",
    "        \n",
    "        # SQLite imports\n",
    "        sqlite_files = [\n",
    "            {\"db\": \"aenc.sqlite\", \"tables\": []},\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        for file in sqlite_files:\n",
    "            db_path = f\"../../data/raw/{file['db']}\"\n",
    "            print(f\"Processing SQLite database: {file['db']}\")\n",
    "            \n",
    "            try:\n",
    "                conn_sqlite = sql.connect(db_path)\n",
    "                \n",
    "                # Dynamically fetch all table names from the SQLite database\n",
    "                cursor_sqlite = conn_sqlite.cursor()\n",
    "                cursor_sqlite.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "                tables = [row[0] for row in cursor_sqlite.fetchall()]\n",
    "                cursor_sqlite.close()\n",
    "                \n",
    "                print(f\"Found tables: {tables}\")\n",
    "                \n",
    "                for table in tables:\n",
    "                    print(f\"Importing table: {table}\")\n",
    "                    \n",
    "                    # Check if table exists in SQL Server\n",
    "                    if table.upper() not in [t.upper() for t in sql_tables]:\n",
    "                        print(f\"Table {table} not found in SQL Server, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Get column information\n",
    "                    column_types = get_column_types(cursor, table)\n",
    "                    nullable_info = get_nullable_columns(cursor, table)\n",
    "                    \n",
    "                    # Read data from SQLite\n",
    "                    df = pd.read_sql(f\"SELECT * FROM {table}\", conn_sqlite)\n",
    "                    if df.empty:\n",
    "                        print(f\"No data found in {table}, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Get SQL Server columns\n",
    "                    cursor.execute(f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table}'\")\n",
    "                    sql_cols = [row[0] for row in cursor.fetchall()]\n",
    "                    \n",
    "                    # Find common columns\n",
    "                    common_cols = []\n",
    "                    for col in df.columns:\n",
    "                        matching_cols = [s_col for s_col in sql_cols if s_col.upper() == col.upper()]\n",
    "                        if matching_cols:\n",
    "                            common_cols.append((col, matching_cols[0]))  # (sqlite_col, sql_server_col)\n",
    "                    \n",
    "                    if not common_cols:\n",
    "                        print(f\"No matching columns found for {table}, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Prepare SQL statement\n",
    "                    sql_cols_str = \", \".join([f\"[{col[1]}]\" for col in common_cols])\n",
    "                    placeholders = \", \".join([\"?\"] * len(common_cols))\n",
    "                    \n",
    "                    # Standard insert for tables\n",
    "                    for _, row in df.iterrows():\n",
    "                        try:\n",
    "                            # Convert values\n",
    "                            values = []\n",
    "                            for sqlite_col, sql_col in common_cols:\n",
    "                                value = convert_numpy_type(row[sqlite_col])\n",
    "                                col_type = column_types.get(sql_col, 'TEXT')\n",
    "                                converted_value = convert_value_for_column(value, sql_col, col_type, table.upper())\n",
    "                                values.append(converted_value)\n",
    "                            \n",
    "                            # Execute insert\n",
    "                            cursor.execute(f\"INSERT INTO {table} ({sql_cols_str}) VALUES ({placeholders})\", tuple(values))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error inserting row in {table}: {e}\")\n",
    "                    \n",
    "                    # Commit after each table\n",
    "                    conn.commit()\n",
    "                    print(f\"Successfully imported table: {table}\")\n",
    "                \n",
    "                conn_sqlite.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing SQLite database {file['db']}: {e}\")\n",
    "\n",
    "        print(\"Re-enabling constraints...\")\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"ALTER TABLE [{table}] WITH NOCHECK CHECK CONSTRAINT ALL\")\n",
    "        conn.commit()\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"Database import completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error during import: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute the import function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import_data()\n",
    "        print(\"Import completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acces importeren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data transfer from Access to SQL Server...\n",
      "Connecting to Access database at c:\\Users\\veyse\\source\\DEDS_portfolio\\data\\raw\\AdventureWorks.mdb...\n",
      "Connected to Access database successfully\n",
      "Connecting to SQL Server database...\n",
      "Connected to SQL Server database successfully\n",
      "  Disabling constraints for Bonus...\n",
      "  Disabling constraints for Customer...\n",
      "  Disabling constraints for CustomerCustomerDemo...\n",
      "  Disabling constraints for EmployeeTerritories...\n",
      "  Disabling constraints for OrderDetails...\n",
      "  Disabling constraints for Orders...\n",
      "  Disabling constraints for purchaseOrderDetail...\n",
      "  Disabling constraints for purchaseOrderHeader...\n",
      "  Disabling constraints for Sales_Order...\n",
      "  Disabling constraints for sales_order_item...\n",
      "  Disabling constraints for Sales_OrderDetail...\n",
      "  Disabling constraints for Sales_OrderHeader...\n",
      "  Disabling constraints for Sales_Store...\n",
      "  Disabling constraints for Territories...\n",
      "All constraints disabled\n",
      "Found 16 tables in Access database\n",
      "Found 30 tables in SQL Server database\n",
      "Found 15 mapping tables between databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Table: HumanResources_Department -> resources_department\n",
      "  Mapped 4 of 4 columns\n",
      "  Transferred 16 of 16 rows from HumanResources_Department to resources_department\n",
      "  Table: HumanResources_Employee -> Employee\n",
      "  Mapped 16 of 17 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  13%|█▎        | 2/15 [00:00<00:03,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 290 of 290 rows from HumanResources_Employee to Employee\n",
      "  Table: Person_Address -> Address\n",
      "  Mapped 10 of 10 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  20%|██        | 3/15 [00:11<00:58,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 19614 of 19614 rows from Person_Address to Address\n",
      "  Table: Person_Person -> person\n",
      "  Mapped 11 of 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  27%|██▋       | 4/15 [00:21<01:11,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 19972 of 19972 rows from Person_Person to person\n",
      "  Table: Production_BillOfMaterials -> billOfMaterials\n",
      "  Mapped 9 of 9 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  33%|███▎      | 5/15 [00:22<00:46,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 2679 of 2679 rows from Production_BillOfMaterials to billOfMaterials\n",
      "  Table: Production_Product -> Product\n",
      "  Mapped 26 of 26 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  40%|████      | 6/15 [00:23<00:30,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 504 of 504 rows from Production_Product to Product\n",
      "  Table: Production_ProductCategory -> productCategory\n",
      "  Mapped 4 of 4 columns\n",
      "  Transferred 4 of 4 rows from Production_ProductCategory to productCategory\n",
      "  Table: Purchasing_PurchaseOrderDetail -> purchaseOrderDetail\n",
      "  Mapped 11 of 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  53%|█████▎    | 8/15 [00:27<00:18,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 8845 of 8845 rows from Purchasing_PurchaseOrderDetail to purchaseOrderDetail\n",
      "  Table: Purchasing_PurchaseOrderHeader -> purchaseOrderHeader\n",
      "  Mapped 13 of 13 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  67%|██████▋   | 10/15 [00:29<00:09,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 4012 of 4012 rows from Purchasing_PurchaseOrderHeader to purchaseOrderHeader\n",
      "  Table: Purchasing_Vendor -> purchasing_vendor\n",
      "  Mapped 8 of 8 columns\n",
      "  Transferred 104 of 104 rows from Purchasing_Vendor to purchasing_vendor\n",
      "  Table: Sales_Customer -> Customer\n",
      "  Mapped 7 of 7 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  73%|███████▎  | 11/15 [00:37<00:14,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 19820 of 19820 rows from Sales_Customer to Customer\n",
      "  Table: Sales_SalesOrderDetail -> Sales_OrderDetail\n",
      "  Mapped 11 of 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  80%|████████  | 12/15 [01:30<00:52, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 121317 of 121317 rows from Sales_SalesOrderDetail to Sales_OrderDetail\n",
      "  Table: Sales_SalesOrderHeader -> Sales_OrderHeader\n",
      "  Mapped 24 of 26 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables:  93%|█████████▎| 14/15 [01:45<00:12, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 31465 of 31465 rows from Sales_SalesOrderHeader to Sales_OrderHeader\n",
      "  Table: Sales_SalesTerritory -> Sales_territory\n",
      "  Mapped 10 of 10 columns\n",
      "  Transferred 10 of 10 rows from Sales_SalesTerritory to Sales_territory\n",
      "  Table: Sales_Store -> Sales_Store\n",
      "  Mapped 5 of 5 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring tables: 100%|██████████| 15/15 [01:45<00:00,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 701 of 701 rows from Sales_Store to Sales_Store\n",
      "Successfully transferred data for 15 out of 15 tables\n",
      "  Re-enabling constraints for Bonus...\n",
      "Error re-enabling constraints: ('23000', '[23000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The ALTER TABLE statement conflicted with the FOREIGN KEY constraint \"FK__Bonus__emp_id__6EF57B66\". The conflict occurred in database \"SDM_project\", table \"dbo.Employee\", column \\'id\\'. (547) (SQLExecDirectW)')\n",
      "Closing database connections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transfer process completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "access_db_path = os.path.abspath('../../data/raw/AdventureWorks.mdb')\n",
    "\n",
    "# SQL Server connection string\n",
    "sql_server_conn_str = (\n",
    "    'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    'SERVER=LAPTOP-HLMO0COA\\\\SQLEXPRESS;' \n",
    "    'DATABASE=SDM_project;' \n",
    "    'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "# Define the mapping between Access and SQL Server table names\n",
    "table_mapping = {\n",
    "    'HumanResources_Department': 'resources_department',\n",
    "    'HumanResources_Employee': 'Employee',\n",
    "    'Person_Address': 'Address',\n",
    "    'Person_Person': 'person',\n",
    "    'Production_BillOfMaterials': 'billOfMaterials',\n",
    "    'Production_Product': 'Product',\n",
    "    'Production_ProductCategory': 'productCategory',\n",
    "    'Purchasing_PurchaseOrderDetail': 'purchaseOrderDetail',\n",
    "    'Purchasing_PurchaseOrderHeader': 'purchaseOrderHeader',\n",
    "    'Purchasing_Vendor': 'purchasing_vendor',\n",
    "    'Sales_Customer': 'Customer',\n",
    "    'Sales_SalesOrderDetail': 'Sales_OrderDetail',\n",
    "    'Sales_SalesOrderHeader': 'Sales_OrderHeader',\n",
    "    'Sales_SalesTerritory': 'Sales_territory',\n",
    "    'Sales_Store': 'Sales_Store'\n",
    "}\n",
    "\n",
    "# Define column mappings for tables where field names differ\n",
    "column_mappings = {\n",
    "    # resources_department\n",
    "    'resources_department': {\n",
    "        'DepartmentID': 'DepartmentID',\n",
    "        'Name': 'Name',\n",
    "        'GroupName': 'GroupName',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # Employee\n",
    "    'Employee': {\n",
    "        'BusinessEntityID': 'BusinessEntityID',\n",
    "        'ss_number': 'NationalIDNumber',\n",
    "        'LoginID': 'LoginID',\n",
    "        'OrganizationNode': 'OrganizationNode',\n",
    "        'OrganizationLevel': 'OrganizationLevel',\n",
    "        'JobTitle': 'JobTitle',\n",
    "        'BirthDate': 'BirthDate',\n",
    "        'maritalstatus': 'MaritalStatus',\n",
    "        'gender': 'Gender',\n",
    "        'HireDate': 'HireDate',\n",
    "        'salariedFlag': 'SalariedFlag',\n",
    "        'vacationHours': 'VacationHours',\n",
    "        'SickleaveHours': 'SickleaveHours',\n",
    "        'currentFlag': 'CurrentFlag',\n",
    "        'rowguid': 'rowguid',\n",
    "        'modifiedDate': 'ModifiedDate',\n",
    "        'departmentid': 'dept_id'\n",
    "    },\n",
    "    # Address\n",
    "    'Address': {\n",
    "        'AddressID': 'AddressID',\n",
    "        'AddressLine1': 'AddressLine1',\n",
    "        'AddressLine2': 'AddressLine2',\n",
    "        'City': 'City',\n",
    "        'StateProvinceID': 'StateProvinceID',\n",
    "        'PostalCode': 'PostalCode',\n",
    "        'SpatialLocation': 'SpatialLocation',\n",
    "        'rowguid': 'rowguid',\n",
    "        'ModifiedDate': 'ModifiedDate',\n",
    "        'BusinessEntityID': 'BusinessEntityID'\n",
    "    },\n",
    "    # person\n",
    "    'person': {\n",
    "        'businessentityid': 'BusinessEntityID',\n",
    "        'persontype': 'PersonType',\n",
    "        'namestyle': 'NameStyle',\n",
    "        'title': 'Title',\n",
    "        'firstname': 'FirstName',\n",
    "        'middlename': 'MiddleName',\n",
    "        'lastname': 'LastName',\n",
    "        'suffix': 'Suffix',\n",
    "        'emailpromotion': 'EmailPromotion',\n",
    "        'rowguid': 'rowguid',\n",
    "        'modifieddate': 'ModifiedDate'\n",
    "    },\n",
    "    # billOfMaterials\n",
    "    'billOfMaterials': {\n",
    "        'BillOfMaterialsID': 'BillOfMaterialsID',\n",
    "        'ProductAssemblyID': 'ProductAssemblyID',\n",
    "        'ComponentID': 'ComponentID',\n",
    "        'StartDate': 'StartDate',\n",
    "        'EndDate': 'EndDate',\n",
    "        'UnitMeasureCode': 'UnitMeasureCode',\n",
    "        'BOMLevel': 'BOMLevel',\n",
    "        'PerAssemblyQty': 'PerAssemblyQty',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # Product\n",
    "    'Product': {\n",
    "        'id': 'ProductID',\n",
    "        'Name': 'Name',\n",
    "        'ProductNumber': 'ProductNumber',\n",
    "        'MakeFlag': 'MakeFlag',\n",
    "        'FinishedGoodsFlag': 'FinishedGoodsFlag',\n",
    "        'Color': 'Color',\n",
    "        'SafetyStockLevel': 'SafetyStockLevel',\n",
    "        'ReorderPoint': 'ReorderPoint',\n",
    "        'StandardCost': 'StandardCost',\n",
    "        'ListPrice': 'ListPrice',\n",
    "        'Size': 'Size',\n",
    "        'SizeUnitMeasureCode': 'SizeUnitMeasureCode',\n",
    "        'WeightUnitMeasureCode': 'WeightUnitMeasureCode',\n",
    "        'Weight': 'Weight',\n",
    "        'DaysToManufacture': 'DaysToManufacture',\n",
    "        'ProductLine': 'ProductLine',\n",
    "        'Class': 'Class',\n",
    "        'Style': 'Style',\n",
    "        'ProductSubcategoryID': 'ProductSubcategoryID',\n",
    "        'productModelID': 'productModelID',\n",
    "        'SellStartDate': 'SellStartDate',\n",
    "        'SellEndDate': 'SellEndDate',\n",
    "        'DiscontinuedDate': 'DiscontinuedDate',\n",
    "        'rowguid': 'rowguid',\n",
    "        'ModifiedDate': 'ModifiedDate',\n",
    "        'productCategoryID': 'ProductCategoryID'\n",
    "    },\n",
    "    # productCategory\n",
    "    'productCategory': {\n",
    "        'productCategoryID': 'ProductCategoryID',\n",
    "        'Name': 'Name',\n",
    "        'rowguid': 'rowguid',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # purchaseOrderDetail\n",
    "    'purchaseOrderDetail': {\n",
    "        'purchaseOrderId': 'PurchaseOrderID',\n",
    "        'purchaseOrderDetailId': 'PurchaseOrderDetailID',\n",
    "        'dueDate': 'DueDate',\n",
    "        'orderQty': 'OrderQty',\n",
    "        'productId': 'ProductID',\n",
    "        'unitPrice': 'UnitPrice',\n",
    "        'lineTotal': 'LineTotal',\n",
    "        'receivedQty': 'ReceivedQty',\n",
    "        'rejectedQty': 'RejectedQty',\n",
    "        'stockedQty': 'StockedQty',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # purchaseOrderHeader\n",
    "    'purchaseOrderHeader': {\n",
    "        'purchaseOrderId': 'PurchaseOrderID',\n",
    "        'revisionNumber': 'RevisionNumber',\n",
    "        'status': 'Status',\n",
    "        'employeeID': 'EmployeeID',\n",
    "        'vendorID': 'VendorID',\n",
    "        'shipMethodID': 'ShipMethodID',\n",
    "        'orderDate': 'OrderDate',\n",
    "        'shipDate': 'ShipDate',\n",
    "        'subtotal': 'SubTotal',\n",
    "        'taxAmt': 'TaxAmt',\n",
    "        'freight': 'Freight',\n",
    "        'totalDue': 'TotalDue',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # purchasing_vendor\n",
    "    'purchasing_vendor': {\n",
    "        'businessEntityId': 'BusinessEntityID',\n",
    "        'accountNumber': 'AccountNumber',\n",
    "        'name': 'Name',\n",
    "        'creditRating': 'CreditRating',\n",
    "        'preferredVendorStatus': 'PreferredVendorStatus',\n",
    "        'ActiveFlag': 'ActiveFlag',\n",
    "        'purchasingWebServiceURL': 'PurchasingWebServiceURL',\n",
    "        'ModifiedDate': 'ModifiedDate',\n",
    "    },\n",
    "    # Customer\n",
    "    'Customer': {\n",
    "        'id': 'CustomerID',\n",
    "        'PersonID': 'PersonID',\n",
    "        'StoreID': 'StoreID',\n",
    "        'AccountNumber': 'AccountNumber',\n",
    "        'territoryID': 'TerritoryID',\n",
    "        'rowguid': 'rowguid',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # Sales_OrderDetail\n",
    "    'Sales_OrderDetail': {\n",
    "        'salesOrderId': 'SalesOrderID',\n",
    "        'salesOrderDetailId': 'SalesOrderDetailID',\n",
    "        'carrierTrackingNumber': 'CarrierTrackingNumber',\n",
    "        'orderQty': 'OrderQty',\n",
    "        'productId': 'ProductID',\n",
    "        'specialOfferId': 'SpecialOfferID',\n",
    "        'unitPrice': 'UnitPrice',\n",
    "        'unitPriceDiscount': 'UnitPriceDiscount',\n",
    "        'lineTotal': 'LineTotal',\n",
    "        'rowguid': 'rowguid',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # Sales_OrderHeader\n",
    "    'Sales_OrderHeader': {\n",
    "        'SalesOrderID': 'SalesOrderID',\n",
    "        'revisionNumber': 'RevisionNumber',\n",
    "        'orderDate': 'OrderDate',\n",
    "        'dueDate': 'DueDate',\n",
    "        'shipDate': 'ShipDate',\n",
    "        'status': 'Status',\n",
    "        'salesOrderNumber': 'SalesOrderNumber',\n",
    "        'purchaseOrderNumber': 'PurchaseOrderNumber',\n",
    "        'accountNumber': 'AccountNumber',\n",
    "        'customerID': 'CustomerID',\n",
    "        'territoryID': 'TerritoryID',\n",
    "        'billToAddressID': 'BillToAddressID',\n",
    "        'shipToAddressID': 'ShipToAddressID',\n",
    "        'shipMethodID': 'ShipMethodID',\n",
    "        'creditCardID': 'CreditCardID',\n",
    "        'creditCardApprovalCode': 'CreditCardApprovalCode',\n",
    "        'currencyRateID': 'CurrencyRateID',\n",
    "        'subTotal': 'SubTotal',\n",
    "        'taxAmt': 'TaxAmt',\n",
    "        'freight': 'Freight',\n",
    "        'totalDue': 'TotalDue',\n",
    "        'comment': 'Comment',\n",
    "        'rowguid': 'rowguid',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # Sales_territory\n",
    "    'Sales_territory': {\n",
    "        'territoryID': 'TerritoryID',\n",
    "        'name': 'Tame',\n",
    "        'CountryRegionCode': 'CountryRegionCode',\n",
    "        'GroupName': 'Group',\n",
    "        'SalesYTD': 'SalesYTD',\n",
    "        'SalesLastYear': 'SalesLastYear',\n",
    "        'CostYTD': 'CostYTD',\n",
    "        'CostLastYear': 'CostLastYear',\n",
    "        'rowguid': 'rowguid',\n",
    "        'ModifiedDate': 'ModifiedDate'\n",
    "    },\n",
    "    # Sales_Store\n",
    "    'Sales_Store': {\n",
    "        'BusinessEntityId': 'BusinessEntityId',\n",
    "        'Name': 'Name',\n",
    "        'SalesPersonId': 'SalesPersonId',\n",
    "        'rowguid': 'rowguid',\n",
    "        'ModifiedDate': 'ModifiedDate',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to get all tables from Access database\n",
    "def get_access_tables(conn):\n",
    "    cursor = conn.cursor()\n",
    "    tables = []\n",
    "    \n",
    "    for row in cursor.tables(tableType='TABLE'):\n",
    "        if row.table_name.startswith('MSys'):\n",
    "            continue  # Skip system tables\n",
    "        tables.append(row.table_name)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "# Function to disable constraints in SQL Server database\n",
    "def disable_constraints(conn):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all tables with constraints\n",
    "    tables_query = \"\"\"\n",
    "    SELECT DISTINCT OBJECT_NAME(parent_object_id) AS TableName\n",
    "    FROM sys.foreign_keys\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(tables_query)\n",
    "        tables_with_constraints = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        # Disable constraints for each table individually\n",
    "        for table in tables_with_constraints:\n",
    "            print(f\"  Disabling constraints for {table}...\")\n",
    "            cursor.execute(f\"ALTER TABLE [{table}] NOCHECK CONSTRAINT ALL\")\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"All constraints disabled\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error disabling constraints: {str(e)}\")\n",
    "    \n",
    "    return tables_with_constraints\n",
    "\n",
    "# Function to enable constraints in SQL Server database\n",
    "def enable_constraints(conn, tables_with_constraints):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Re-enable constraints for each table individually\n",
    "        for table in tables_with_constraints:\n",
    "            print(f\"  Re-enabling constraints for {table}...\")\n",
    "            cursor.execute(f\"ALTER TABLE [{table}] WITH CHECK CHECK CONSTRAINT ALL\")\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"All constraints re-enabled successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error re-enabling constraints: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to transfer data from Access to SQL Server\n",
    "def transfer_data(access_conn, sql_conn):\n",
    "    access_cursor = access_conn.cursor()\n",
    "    sql_cursor = sql_conn.cursor()\n",
    "    \n",
    "    # Get table names from Access\n",
    "    access_tables = get_access_tables(access_conn)\n",
    "    print(f\"Found {len(access_tables)} tables in Access database\")\n",
    "    \n",
    "    # Get table names from SQL Server\n",
    "    sql_cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n",
    "    sql_tables = [row[0] for row in sql_cursor.fetchall()]\n",
    "    print(f\"Found {len(sql_tables)} tables in SQL Server database\")\n",
    "    \n",
    "    # Match tables between the two databases using the mapping\n",
    "    mapped_tables = []\n",
    "    for access_table in access_tables:\n",
    "        # First check if there's a specific mapping for this table\n",
    "        if access_table in table_mapping:\n",
    "            sql_table = table_mapping[access_table]\n",
    "            if sql_table in sql_tables:\n",
    "                mapped_tables.append((access_table, sql_table))\n",
    "                continue\n",
    "        \n",
    "        # If no mapping or mapped table doesn't exist, try case-insensitive match\n",
    "        matches = [sql_table for sql_table in sql_tables if sql_table.lower() == access_table.lower()]\n",
    "        if matches:\n",
    "            mapped_tables.append((access_table, matches[0]))\n",
    "    \n",
    "    print(f\"Found {len(mapped_tables)} mapping tables between databases\")\n",
    "    \n",
    "    # Process each matching table\n",
    "    successful_transfers = 0\n",
    "    for access_table, sql_table in tqdm(mapped_tables, desc=\"Transferring tables\"):\n",
    "        try:\n",
    "            # Get column information from SQL Server table\n",
    "            sql_cursor.execute(f\"SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{sql_table}'\")\n",
    "            sql_columns_info = {row[0].lower(): row[1] for row in sql_cursor.fetchall()}\n",
    "            sql_columns = list(sql_columns_info.keys())\n",
    "            \n",
    "            # Get data from Access table\n",
    "            access_cursor.execute(f\"SELECT * FROM [{access_table}]\")\n",
    "            access_columns = [column[0].lower() for column in access_cursor.description]\n",
    "            \n",
    "            # Find matching columns using column mappings if available\n",
    "            column_map = {}\n",
    "            if sql_table in column_mappings:\n",
    "                # Use defined column mappings\n",
    "                for access_col in access_columns:\n",
    "                    # Try to find a mapping for this column\n",
    "                    mapped_col = None\n",
    "                    for sql_col, access_col_name in column_mappings[sql_table].items():\n",
    "                        if access_col.lower() == access_col_name.lower():\n",
    "                            mapped_col = sql_col\n",
    "                            break\n",
    "                    \n",
    "                    # If mapping found and mapped column exists in SQL table\n",
    "                    if mapped_col and mapped_col.lower() in sql_columns:\n",
    "                        column_map[access_col] = mapped_col\n",
    "                    # Otherwise try direct match\n",
    "                    elif access_col.lower() in sql_columns:\n",
    "                        column_map[access_col] = access_col\n",
    "            else:\n",
    "                # If no explicit mapping, try direct case-insensitive match\n",
    "                for access_col in access_columns:\n",
    "                    for sql_col in sql_columns:\n",
    "                        if access_col.lower() == sql_col.lower():\n",
    "                            column_map[access_col] = sql_col\n",
    "                            break\n",
    "            \n",
    "            # Get matching columns (Access columns that have a mapping to SQL columns)\n",
    "            matching_columns = list(column_map.keys())\n",
    "            \n",
    "            print(f\"  Table: {access_table} -> {sql_table}\")\n",
    "            print(f\"  Mapped {len(matching_columns)} of {len(access_columns)} columns\")\n",
    "            \n",
    "            # Prepare data for insertion\n",
    "            rows = []\n",
    "            for row in access_cursor.fetchall():\n",
    "                # Create a dictionary with only the matching columns\n",
    "                row_dict = {}\n",
    "                for i, col in enumerate(access_columns):\n",
    "                    if col in matching_columns:\n",
    "                        row_dict[col] = row[i]\n",
    "                rows.append(row_dict)\n",
    "            \n",
    "            if rows:\n",
    "                # Prepare insert statement with mapped column names\n",
    "                sql_cols = [column_map[col] for col in matching_columns]\n",
    "                placeholders = ', '.join(['?' for _ in matching_columns])\n",
    "                columns_str = ', '.join([f'[{col}]' for col in sql_cols])\n",
    "                insert_query = f\"INSERT INTO [{sql_table}] ({columns_str}) VALUES ({placeholders})\"\n",
    "                \n",
    "                # Insert batch of rows\n",
    "                success_count = 0\n",
    "                for row_dict in rows:\n",
    "                    values = [row_dict[col] for col in matching_columns]\n",
    "                    try:\n",
    "                        sql_cursor.execute(insert_query, values)\n",
    "                        success_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error inserting into {sql_table}: {str(e)}\")\n",
    "                        print(f\"  Query: {insert_query}\")\n",
    "                        print(f\"  Values: {values}\")\n",
    "                        continue\n",
    "                \n",
    "                sql_conn.commit()\n",
    "                successful_transfers += 1\n",
    "                print(f\"  Transferred {success_count} of {len(rows)} rows from {access_table} to {sql_table}\")\n",
    "            else:\n",
    "                print(f\"  No data found in {access_table}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing table {access_table}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Successfully transferred data for {successful_transfers} out of {len(mapped_tables)} tables\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    print(\"Starting data transfer from Access to SQL Server...\")\n",
    "    \n",
    "    # Try to connect to Access database\n",
    "    try:\n",
    "        print(f\"Connecting to Access database at {access_db_path}...\")\n",
    "        if not os.path.exists(access_db_path):\n",
    "            print(f\"Error: Access database file not found at {access_db_path}\")\n",
    "            return\n",
    "        \n",
    "        access_conn_str = (\n",
    "            r\"Driver={Microsoft Access Driver (*.mdb, *.accdb)};\"\n",
    "            r\"DBQ=\" + access_db_path + \";\"\n",
    "        )\n",
    "        access_conn = pyodbc.connect(access_conn_str)\n",
    "        print(\"Connected to Access database successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Access database: {str(e)}\")\n",
    "        print(\"\\nAvailable ODBC drivers:\")\n",
    "        for driver in pyodbc.drivers():\n",
    "            print(f\"  - {driver}\")\n",
    "        return\n",
    "    \n",
    "    # Try to connect to SQL Server database\n",
    "    try:\n",
    "        print(\"Connecting to SQL Server database...\")\n",
    "        sql_conn = pyodbc.connect(sql_server_conn_str)\n",
    "        print(\"Connected to SQL Server database successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to SQL Server database: {str(e)}\")\n",
    "        access_conn.close()\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Disable constraints\n",
    "        tables_with_constraints = disable_constraints(sql_conn)\n",
    "        \n",
    "        # Transfer data\n",
    "        transfer_data(access_conn, sql_conn)\n",
    "        \n",
    "        # Re-enable constraints\n",
    "        enable_constraints(sql_conn, tables_with_constraints)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during data transfer: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Close connections\n",
    "        print(\"Closing database connections...\")\n",
    "        access_conn.close()\n",
    "        sql_conn.close()\n",
    "        print(\"Data transfer process completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Northwind(SQL Server) importeren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Northwind to SDM_project data transfer...\n",
      "Connecting to Northwind database...\n",
      "Connected to Northwind database successfully\n",
      "Connecting to SDM_project database...\n",
      "Connected to SDM_project database successfully\n",
      "  Disabling constraints for Bonus...\n",
      "  Disabling constraints for Customer...\n",
      "  Disabling constraints for CustomerCustomerDemo...\n",
      "  Disabling constraints for EmployeeTerritories...\n",
      "  Disabling constraints for OrderDetails...\n",
      "  Disabling constraints for Orders...\n",
      "  Disabling constraints for purchaseOrderDetail...\n",
      "  Disabling constraints for purchaseOrderHeader...\n",
      "  Disabling constraints for Sales_Order...\n",
      "  Disabling constraints for sales_order_item...\n",
      "  Disabling constraints for Sales_OrderDetail...\n",
      "  Disabling constraints for Sales_OrderHeader...\n",
      "  Disabling constraints for Sales_Store...\n",
      "  Disabling constraints for Territories...\n",
      "All constraints disabled\n",
      "\n",
      "Processing base tables...\n",
      "Copying data from Categories to Categories\n",
      "  Identity columns in Categories: []\n",
      "  Primary key columns in Categories: ['CategoryID']\n",
      "  Mapped columns: CategoryID, CategoryName, Description, Picture -> CategoryID, CategoryName, Description, Picture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Categories: 100%|██████████| 8/8 [00:00<00:00, 324.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 8 new rows to Categories (Skipped 0 rows)\n",
      "Copying data from Customers to Customer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Identity columns in Customer: ['customerid']\n",
      "  Primary key columns in Customer: ['CustomerId']\n",
      "  Mapped columns: CustomerID, CompanyName, ContactName, Address, City, Region, PostalCode, Country, Phone, Fax -> id, company_name, name, address, city, state, zip, Country, phone, fax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Customer: 100%|██████████| 91/91 [00:00<00:00, 353.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 91 new rows to Customer (Skipped 0 rows)\n",
      "Copying data from Employees to Employee\n",
      "  Identity columns in Employee: ['id']\n",
      "  Primary key columns in Employee: ['id']\n",
      "  Warning: Mapped column title not found in destination table\n",
      "  Mapped columns: EmployeeID, LastName, FirstName, Title, BirthDate, HireDate, City, Region, PostalCode, Country, HomePhone, Extension, Photo, Notes, ReportsTo, PhotoPath -> emp_id, emp_lname, emp_fname, JobTitle, BirthDate, HireDate, City, Region, Postal-code, country, home_phone, extension, Photo, Notes, ReportsTo, photopath\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Employee: 100%|██████████| 9/9 [00:00<00:00, 224.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 9 new rows to Employee (Skipped 0 rows)\n",
      "Copying data from Shippers to Shippers\n",
      "  Identity columns in Shippers: []\n",
      "  Primary key columns in Shippers: ['ShipperID']\n",
      "  Mapped columns: ShipperID, CompanyName, Phone -> ShipperID, CompanyName, Phone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Shippers: 100%|██████████| 3/3 [00:00<00:00, 426.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 3 new rows to Shippers (Skipped 0 rows)\n",
      "Copying data from Suppliers to Suppliers\n",
      "  Identity columns in Suppliers: []\n",
      "  Primary key columns in Suppliers: ['SupplierID']\n",
      "  Mapped columns: SupplierID, CompanyName, ContactName, ContactTitle, Address, City, Region, PostalCode, Country, Phone, Fax, HomePage -> SupplierID, CompanyName, ContactName, ContactTitle, Address, City, Region, PostalCode, Country, Phone, Fax, HomePage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Suppliers: 100%|██████████| 29/29 [00:00<00:00, 247.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 29 new rows to Suppliers (Skipped 0 rows)\n",
      "Copying data from Region to Region\n",
      "  Identity columns in Region: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Primary key columns in Region: ['RegionID']\n",
      "  Mapped columns: RegionID, RegionDescription -> RegionID, RegionDescription\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Region: 100%|██████████| 4/4 [00:00<00:00, 601.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 4 new rows to Region (Skipped 0 rows)\n",
      "\n",
      "Processing dependent tables...\n",
      "Copying data from Products to Product\n",
      "  Identity columns in Product: ['productid']\n",
      "  Primary key columns in Product: ['productId']\n",
      "  Mapped columns: ProductID, ProductName, SupplierID, CategoryID, QuantityPerUnit, UnitPrice, UnitsInStock, UnitsOnOrder, ReorderLevel, Discontinued -> id, Name, SupplierID, productCategoryID, Description, UnitPrice, UnitsInStock, UnitsOnOrder, ReorderLevel, Discontinued\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Product: 100%|██████████| 77/77 [00:00<00:00, 425.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 77 new rows to Product (Skipped 0 rows)\n",
      "Copying data from Orders to Orders\n",
      "  Identity columns in Orders: []\n",
      "  Primary key columns in Orders: ['OrderID']\n",
      "  Mapped columns: OrderID, EmployeeID, OrderDate, RequiredDate, ShippedDate, ShipVia, Freight, ShipName, ShipAddress, ShipCity, ShipRegion, ShipPostalCode, ShipCountry, CustomerID -> OrderID, EmployeeID, OrderDate, RequiredDate, ShippedDate, ShipVia, Freight, ShipName, ShipAddress, ShipCity, ShipRegion, ShipPostalCode, ShipCountry, CustomerID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Orders: 100%|██████████| 830/830 [00:01<00:00, 476.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 830 new rows to Orders (Skipped 0 rows)\n",
      "Copying data from Territories to Territories\n",
      "  Identity columns in Territories: []\n",
      "  Primary key columns in Territories: ['TerritoryID']\n",
      "  Mapped columns: TerritoryID, TerritoryDescription, RegionID -> TerritoryID, TerritoryDescription, RegionID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into Territories: 100%|██████████| 53/53 [00:00<00:00, 1299.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 53 new rows to Territories (Skipped 0 rows)\n",
      "Copying data from [Order Details] to OrderDetails\n",
      "  Identity columns in OrderDetails: []\n",
      "  Primary key columns in OrderDetails: ['OrderID', 'ProductID']\n",
      "  Mapped columns: OrderID, ProductID, UnitPrice, Quantity, Discount -> OrderID, ProductID, UnitPrice, Quantity, Discount\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into OrderDetails: 100%|██████████| 2155/2155 [00:01<00:00, 1316.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 2155 new rows to OrderDetails (Skipped 0 rows)\n",
      "Copying data from EmployeeTerritories to EmployeeTerritories\n",
      "  Identity columns in EmployeeTerritories: []\n",
      "  Primary key columns in EmployeeTerritories: ['EmployeeID', 'TerritoryID']\n",
      "  Mapped columns: EmployeeID, TerritoryID -> EmployeeID, TerritoryID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Inserting into EmployeeTerritories: 100%|██████████| 49/49 [00:00<00:00, 1253.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transferred 49 new rows to EmployeeTerritories (Skipped 0 rows)\n",
      "  Re-enabling constraints for Bonus...\n",
      "Error re-enabling constraints: ('23000', '[23000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The ALTER TABLE statement conflicted with the FOREIGN KEY constraint \"FK__Bonus__emp_id__6EF57B66\". The conflict occurred in database \"SDM_project\", table \"dbo.Employee\", column \\'id\\'. (547) (SQLExecDirectW)')\n",
      "Data transfer completed successfully\n",
      "Database connections closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# SQL Server connection string for Northwind (source)\n",
    "northwind_conn_str = (\n",
    "    r\"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    r\"SERVER=LAPTOP-HLMO0COA\\SQLEXPRESS;\"\n",
    "    r\"DATABASE=northwind;\"\n",
    "    r\"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "# SQL Server connection string for SDM_project (destination)\n",
    "sdm_conn_str = (\n",
    "    'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    'SERVER=LAPTOP-HLMO0COA\\\\SQLEXPRESS;' \n",
    "    'DATABASE=SDM_project;' \n",
    "    'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "# Define the tables to transfer and their mapping to SDM_project tables\n",
    "tables_to_transfer = {\n",
    "    'Categories': 'Categories',\n",
    "    'Customers': 'Customer',  # Note the different name in SDM_project\n",
    "    'Employees': 'Employee',\n",
    "    'Shippers': 'Shippers',\n",
    "    'Suppliers': 'Suppliers',\n",
    "    'Orders': 'Orders',\n",
    "    'Products': 'Product',  # Note the different name in SDM_project\n",
    "    '[Order Details]': 'OrderDetails',  # Note the different name in SDM_project\n",
    "    'Region': 'Region',\n",
    "    'Territories': 'Territories',\n",
    "    'EmployeeTerritories': 'EmployeeTerritories'\n",
    "}\n",
    "\n",
    "# Define field mapping for tables with different column names or structures\n",
    "field_mappings = {\n",
    "    'Customers': {\n",
    "        'CustomerID': 'id',  # Northwind CustomerID maps to id field in SDM_project\n",
    "        'CompanyName': 'company_name',\n",
    "        'ContactName': 'name',\n",
    "        'Phone': 'phone',\n",
    "        'Fax': 'fax',\n",
    "        'Address': 'address',\n",
    "        'City': 'city',\n",
    "        'Region': 'state',\n",
    "        'PostalCode': 'zip',\n",
    "        'Country': 'Country'\n",
    "    },\n",
    "    'Employees': {\n",
    "        'EmployeeID': 'emp_id',\n",
    "        'LastName': 'emp_lname',\n",
    "        'FirstName': 'emp_fname',\n",
    "        'Title': 'JobTitle',\n",
    "        'TitleOfCourtesy': 'title',\n",
    "        'BirthDate': 'BirthDate',\n",
    "        'HireDate': 'HireDate',\n",
    "        'Address': None,  # No direct mapping\n",
    "        'City': 'City',\n",
    "        'Region': 'Region',\n",
    "        'PostalCode': 'Postal-code',\n",
    "        'Country': 'country',\n",
    "        'HomePhone': 'home_phone',\n",
    "        'Extension': 'extension',\n",
    "        'Notes': 'Notes',\n",
    "        'ReportsTo': 'ReportsTo',\n",
    "        'PhotoPath': 'photopath'\n",
    "    },\n",
    "    'Products': {\n",
    "        'ProductID': 'id',  # Northwind ProductID maps to id field in SDM_project\n",
    "        'ProductName': 'Name',\n",
    "        'SupplierID': 'SupplierID',\n",
    "        'CategoryID': 'productCategoryID',\n",
    "        'QuantityPerUnit': 'Description',\n",
    "        'UnitPrice': 'UnitPrice',\n",
    "        'UnitsInStock': 'UnitsInStock',\n",
    "        'UnitsOnOrder': 'UnitsOnOrder',\n",
    "        'ReorderLevel': 'ReorderLevel',\n",
    "        'Discontinued': 'Discontinued'\n",
    "    },\n",
    "    'Orders': {\n",
    "        'OrderID': 'OrderID',\n",
    "        'CustomerID': None,  # Special handling needed - CustomerID in Northwind is string, in SDM_project is int\n",
    "        'OrderDate': 'OrderDate',\n",
    "        'RequiredDate': 'RequiredDate',\n",
    "        'ShippedDate': 'ShippedDate'\n",
    "    },\n",
    "    '[Order Details]': {\n",
    "        'OrderID': 'OrderID',\n",
    "        'ProductID': 'ProductID',\n",
    "        'UnitPrice': 'UnitPrice',\n",
    "        'Quantity': 'Quantity',\n",
    "        'Discount': 'Discount'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to disable constraints in SQL Server database\n",
    "def disable_constraints(conn):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all tables with constraints\n",
    "    tables_query = \"\"\"\n",
    "    SELECT DISTINCT OBJECT_NAME(parent_object_id) AS TableName\n",
    "    FROM sys.foreign_keys\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(tables_query)\n",
    "        tables_with_constraints = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        # Disable constraints for each table individually\n",
    "        for table in tables_with_constraints:\n",
    "            print(f\"  Disabling constraints for {table}...\")\n",
    "            cursor.execute(f\"ALTER TABLE [{table}] NOCHECK CONSTRAINT ALL\")\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"All constraints disabled\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error disabling constraints: {str(e)}\")\n",
    "    \n",
    "    return tables_with_constraints\n",
    "\n",
    "# Function to enable constraints in SQL Server database\n",
    "def enable_constraints(conn, tables_with_constraints):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Re-enable constraints for each table individually\n",
    "        for table in tables_with_constraints:\n",
    "            print(f\"  Re-enabling constraints for {table}...\")\n",
    "            cursor.execute(f\"ALTER TABLE [{table}] WITH CHECK CHECK CONSTRAINT ALL\")\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"All constraints re-enabled successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error re-enabling constraints: {str(e)}\")\n",
    "\n",
    "# Function to check if a table exists\n",
    "def table_exists(conn, table_name):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM INFORMATION_SCHEMA.TABLES \n",
    "        WHERE TABLE_NAME = '{table_name.replace('[', '').replace(']', '')}'\n",
    "    \"\"\")\n",
    "    return cursor.fetchone()[0] > 0\n",
    "\n",
    "# Function to copy table schema\n",
    "def copy_table_schema(source_conn, dest_conn, table_name):\n",
    "    print(f\"Copying schema for table: {table_name}\")\n",
    "    \n",
    "    source_cursor = source_conn.cursor()\n",
    "    dest_cursor = dest_conn.cursor()\n",
    "    \n",
    "    # Get column information\n",
    "    source_cursor.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, \n",
    "               IS_NULLABLE, COLUMN_DEFAULT\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_NAME = '{table_name.replace('[', '').replace(']', '')}'\n",
    "        ORDER BY ORDINAL_POSITION\n",
    "    \"\"\")\n",
    "    \n",
    "    columns = source_cursor.fetchall()\n",
    "    \n",
    "    # Get primary key information\n",
    "    source_cursor.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME\n",
    "        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
    "        WHERE OBJECTPROPERTY(OBJECT_ID(CONSTRAINT_NAME), 'IsPrimaryKey') = 1\n",
    "        AND TABLE_NAME = '{table_name.replace('[', '').replace(']', '')}'\n",
    "    \"\"\")\n",
    "    \n",
    "    primary_keys = [row[0] for row in source_cursor.fetchall()]\n",
    "    \n",
    "    # Construct CREATE TABLE statement\n",
    "    create_table_sql = f\"CREATE TABLE {table_name} (\\n\"\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        col_name, data_type, max_length, is_nullable, default_val = col\n",
    "        \n",
    "        # Format column definition\n",
    "        create_table_sql += f\"    [{col_name}] {data_type}\"\n",
    "        \n",
    "        # Add length for certain data types\n",
    "        if max_length is not None and max_length != -1 and data_type.lower() in ('char', 'varchar', 'nchar', 'nvarchar'):\n",
    "            create_table_sql += f\"({max_length})\"\n",
    "        \n",
    "        # Add NULL/NOT NULL\n",
    "        if is_nullable == 'NO':\n",
    "            create_table_sql += \" NOT NULL\"\n",
    "        else:\n",
    "            create_table_sql += \" NULL\"\n",
    "        \n",
    "        # Add default value if exists\n",
    "        if default_val:\n",
    "            create_table_sql += f\" DEFAULT {default_val}\"\n",
    "        \n",
    "        # Add comma if not the last column\n",
    "        if i < len(columns) - 1:\n",
    "            create_table_sql += \",\\n\"\n",
    "    \n",
    "    # Add primary key constraint if exists\n",
    "    if primary_keys:\n",
    "        pk_columns = \", \".join([f\"[{pk}]\" for pk in primary_keys])\n",
    "        create_table_sql += f\",\\n    CONSTRAINT [PK_{table_name.replace('[', '').replace(']', '')}] PRIMARY KEY CLUSTERED ({pk_columns})\"\n",
    "    \n",
    "    create_table_sql += \"\\n)\"\n",
    "    \n",
    "    try:\n",
    "        # Create the table in destination\n",
    "        dest_cursor.execute(create_table_sql)\n",
    "        dest_conn.commit()\n",
    "        print(f\"  Table {table_name} created successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  Error creating table {table_name}: {str(e)}\")\n",
    "        print(f\"  SQL: {create_table_sql}\")\n",
    "        return False\n",
    "\n",
    "# Function to copy data between tables with field mapping\n",
    "def copy_table_data(source_conn, dest_conn, source_table, dest_table):\n",
    "    print(f\"Copying data from {source_table} to {dest_table}\")\n",
    "    \n",
    "    source_cursor = source_conn.cursor()\n",
    "    dest_cursor = dest_conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Get column names from source\n",
    "        source_cursor.execute(f\"SELECT * FROM {source_table} WHERE 1=0\")\n",
    "        source_columns = [column[0] for column in source_cursor.description]\n",
    "        \n",
    "        # Get column names from destination\n",
    "        dest_cursor.execute(f\"SELECT * FROM {dest_table} WHERE 1=0\")\n",
    "        dest_columns = [column[0] for column in dest_cursor.description]\n",
    "        dest_columns_lower = [col.lower() for col in dest_columns]\n",
    "        \n",
    "        # Check for identity columns in destination\n",
    "        dest_cursor.execute(f\"\"\"\n",
    "            SELECT COLUMN_NAME \n",
    "            FROM INFORMATION_SCHEMA.COLUMNS \n",
    "            WHERE TABLE_NAME = '{dest_table.replace('[', '').replace(']', '')}' \n",
    "            AND COLUMNPROPERTY(OBJECT_ID('{dest_table.replace('[', '').replace(']', '')}'), COLUMN_NAME, 'IsIdentity') = 1\n",
    "        \"\"\")\n",
    "        identity_columns = [row[0].lower() for row in dest_cursor.fetchall()]\n",
    "        print(f\"  Identity columns in {dest_table}: {identity_columns}\")\n",
    "        \n",
    "        # Get primary key info for destination table\n",
    "        dest_cursor.execute(f\"\"\"\n",
    "            SELECT COLUMN_NAME\n",
    "            FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
    "            WHERE OBJECTPROPERTY(OBJECT_ID(CONSTRAINT_NAME), 'IsPrimaryKey') = 1\n",
    "            AND TABLE_NAME = '{dest_table.replace('[', '').replace(']', '')}'\n",
    "        \"\"\")\n",
    "        pk_columns = [row[0] for row in dest_cursor.fetchall()]\n",
    "        print(f\"  Primary key columns in {dest_table}: {pk_columns}\")\n",
    "        \n",
    "        # Determine columns to insert and their mapping\n",
    "        mapped_columns = []\n",
    "        target_columns = []\n",
    "        custom_transforms = {}\n",
    "        \n",
    "        # Check if we have a specific mapping for this table\n",
    "        if source_table in field_mappings:\n",
    "            mapping = field_mappings[source_table]\n",
    "            \n",
    "            for src_col in source_columns:\n",
    "                # Skip identity columns unless we explicitly want to include them\n",
    "                if src_col in mapping:\n",
    "                    dest_col = mapping[src_col]\n",
    "                    if dest_col is not None:\n",
    "                        # Check if destination column exists\n",
    "                        if dest_col.lower() in dest_columns_lower:\n",
    "                            mapped_columns.append(src_col)\n",
    "                            target_columns.append(dest_col)\n",
    "                        else:\n",
    "                            print(f\"  Warning: Mapped column {dest_col} not found in destination table\")\n",
    "                else:\n",
    "                    # Try direct match for columns not in mapping\n",
    "                    if src_col.lower() in dest_columns_lower:\n",
    "                        idx = dest_columns_lower.index(src_col.lower())\n",
    "                        mapped_columns.append(src_col)\n",
    "                        target_columns.append(dest_columns[idx])\n",
    "        else:\n",
    "            # No specific mapping, try direct matches\n",
    "            for src_col in source_columns:\n",
    "                if src_col.lower() in dest_columns_lower:\n",
    "                    idx = dest_columns_lower.index(src_col.lower())\n",
    "                    mapped_columns.append(src_col)\n",
    "                    target_columns.append(dest_columns[idx])\n",
    "        \n",
    "        # Handle special case for CustomerID in Orders table (string to int conversion)\n",
    "        if source_table == 'Orders' and 'CustomerID' in source_columns:\n",
    "            # For Orders table, we need to map CustomerID through the Customer table\n",
    "            # First, create a mapping from Northwind CustomerID (string) to SDM_project CustomerID (int)\n",
    "            source_cursor.execute(\"SELECT CustomerID FROM Customers\")\n",
    "            northwind_customer_ids = [row[0] for row in source_cursor.fetchall()]\n",
    "            \n",
    "            customer_id_map = {}\n",
    "            for n_cust_id in northwind_customer_ids:\n",
    "                # We want to find the Customer record where id = n_cust_id\n",
    "                dest_cursor.execute(f\"SELECT CustomerId FROM Customer WHERE id = '{n_cust_id}'\")\n",
    "                result = dest_cursor.fetchone()\n",
    "                if result:\n",
    "                    customer_id_map[n_cust_id] = result[0]\n",
    "            \n",
    "            custom_transforms['CustomerID'] = lambda x: customer_id_map.get(x, None) if x else None\n",
    "            if 'CustomerID' not in mapped_columns:\n",
    "                mapped_columns.append('CustomerID')\n",
    "                target_columns.append('CustomerID')\n",
    "                \n",
    "        # Handle special case for ProductID in Order Details table\n",
    "        if source_table == '[Order Details]' and 'ProductID' in source_columns:\n",
    "            # Create a mapping from Northwind ProductID to SDM_project productId\n",
    "            source_cursor.execute(\"SELECT ProductID FROM Products\")\n",
    "            northwind_product_ids = [row[0] for row in source_cursor.fetchall()]\n",
    "            \n",
    "            product_id_map = {}\n",
    "            for n_prod_id in northwind_product_ids:\n",
    "                # We want to find the Product record where id = n_prod_id\n",
    "                dest_cursor.execute(f\"SELECT productId FROM Product WHERE id = '{n_prod_id}'\")\n",
    "                result = dest_cursor.fetchone()\n",
    "                if result:\n",
    "                    product_id_map[n_prod_id] = result[0]\n",
    "            \n",
    "            custom_transforms['ProductID'] = lambda x: product_id_map.get(x, None) if x else None\n",
    "        \n",
    "        # Check if we have any columns to insert\n",
    "        if not mapped_columns:\n",
    "            print(f\"  No matching columns found between {source_table} and {dest_table}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"  Mapped columns: {', '.join(mapped_columns)} -> {', '.join(target_columns)}\")\n",
    "        \n",
    "        # Get data from source\n",
    "        mapped_cols_str = \", \".join([f\"[{col}]\" for col in mapped_columns])\n",
    "        source_cursor.execute(f\"SELECT {mapped_cols_str} FROM {source_table}\")\n",
    "        rows = source_cursor.fetchall()\n",
    "        \n",
    "        if not rows:\n",
    "            print(f\"  No data found in {source_table}\")\n",
    "            return True\n",
    "        \n",
    "        # If any target column is an identity column, enable identity insert\n",
    "        identity_insert_needed = any(col.lower() in identity_columns for col in target_columns)\n",
    "        if identity_insert_needed:\n",
    "            try:\n",
    "                dest_cursor.execute(f\"SET IDENTITY_INSERT {dest_table} ON\")\n",
    "                print(f\"  IDENTITY_INSERT enabled for {dest_table}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Failed to enable IDENTITY_INSERT: {str(e)}\")\n",
    "        \n",
    "        # Prepare insert statement\n",
    "        target_cols_str = \", \".join([f\"[{col}]\" for col in target_columns])\n",
    "        placeholders = \", \".join([\"?\" for _ in target_columns])\n",
    "        insert_sql = f\"INSERT INTO {dest_table} ({target_cols_str}) VALUES ({placeholders})\"\n",
    "        \n",
    "        # Insert batch of rows\n",
    "        success_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for row in tqdm(rows, desc=f\"  Inserting into {dest_table}\"):\n",
    "            # Apply any custom transformations\n",
    "            transformed_row = list(row)\n",
    "            for i, col in enumerate(mapped_columns):\n",
    "                if col in custom_transforms:\n",
    "                    transformed_row[i] = custom_transforms[col](row[i])\n",
    "            \n",
    "            # Skip rows with null values for required columns\n",
    "            if any(transformed_row[i] is None and target_columns[i] in pk_columns for i in range(len(transformed_row))):\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if row already exists (if table has primary keys)\n",
    "            if pk_columns:\n",
    "                pk_where = []\n",
    "                pk_values = []\n",
    "                \n",
    "                for i, col in enumerate(target_columns):\n",
    "                    if col in pk_columns:\n",
    "                        if transformed_row[i] is None:\n",
    "                            pk_where.append(f\"[{col}] IS NULL\")\n",
    "                        else:\n",
    "                            pk_where.append(f\"[{col}] = ?\")\n",
    "                            pk_values.append(transformed_row[i])\n",
    "                \n",
    "                if pk_where:\n",
    "                    check_sql = f\"SELECT COUNT(*) FROM {dest_table} WHERE {' AND '.join(pk_where)}\"\n",
    "                    try:\n",
    "                        dest_cursor.execute(check_sql, pk_values)\n",
    "                        exists = dest_cursor.fetchone()[0] > 0\n",
    "                        \n",
    "                        if exists:\n",
    "                            skipped_count += 1\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error checking for existing row: {str(e)}\")\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "            \n",
    "            # Execute the insert\n",
    "            try:\n",
    "                dest_cursor.execute(insert_sql, transformed_row)\n",
    "                success_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error inserting row: {str(e)}\")\n",
    "                skipped_count += 1\n",
    "        \n",
    "        # Turn off identity insert if we turned it on\n",
    "        if identity_insert_needed:\n",
    "            try:\n",
    "                dest_cursor.execute(f\"SET IDENTITY_INSERT {dest_table} OFF\")\n",
    "                print(f\"  IDENTITY_INSERT disabled for {dest_table}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Failed to disable IDENTITY_INSERT: {str(e)}\")\n",
    "        \n",
    "        dest_conn.commit()\n",
    "        print(f\"  Transferred {success_count} new rows to {dest_table} (Skipped {skipped_count} rows)\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error copying data from {source_table} to {dest_table}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Function to add foreign key constraints\n",
    "def add_foreign_keys(source_conn, dest_conn):\n",
    "    print(\"Adding foreign key constraints...\")\n",
    "    \n",
    "    source_cursor = source_conn.cursor()\n",
    "    dest_cursor = dest_conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Get foreign key information from source\n",
    "        source_cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                fk.name AS FKName,\n",
    "                OBJECT_NAME(fk.parent_object_id) AS TableName,\n",
    "                COL_NAME(fkc.parent_object_id, fkc.parent_column_id) AS ColumnName,\n",
    "                OBJECT_NAME(fk.referenced_object_id) AS ReferenceTableName,\n",
    "                COL_NAME(fkc.referenced_object_id, fkc.referenced_column_id) AS ReferenceColumnName\n",
    "            FROM \n",
    "                sys.foreign_keys AS fk\n",
    "                INNER JOIN sys.foreign_key_columns AS fkc ON fk.object_id = fkc.constraint_object_id\n",
    "            ORDER BY\n",
    "                TableName, ColumnName\n",
    "        \"\"\")\n",
    "        \n",
    "        foreign_keys = source_cursor.fetchall()\n",
    "        \n",
    "        for fk in foreign_keys:\n",
    "            fk_name, table_name, column_name, ref_table_name, ref_column_name = fk\n",
    "            \n",
    "            # Check if both tables exist in destination\n",
    "            if not table_exists(dest_conn, table_name) or not table_exists(dest_conn, ref_table_name):\n",
    "                print(f\"  Skipping foreign key {fk_name}: Table(s) not found\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Create foreign key constraint\n",
    "                fk_sql = f\"\"\"\n",
    "                ALTER TABLE [{table_name}] WITH CHECK ADD CONSTRAINT [{fk_name}]\n",
    "                FOREIGN KEY ([{column_name}]) REFERENCES [{ref_table_name}] ([{ref_column_name}])\n",
    "                \"\"\"\n",
    "                \n",
    "                dest_cursor.execute(fk_sql)\n",
    "                dest_conn.commit()\n",
    "                print(f\"  Added foreign key {fk_name}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  Error adding foreign key {fk_name}: {str(e)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting foreign key information: {str(e)}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    print(\"Starting Northwind to SDM_project data transfer...\")\n",
    "    \n",
    "    # Connect to source database (Northwind)\n",
    "    try:\n",
    "        print(\"Connecting to Northwind database...\")\n",
    "        source_conn = pyodbc.connect(northwind_conn_str)\n",
    "        print(\"Connected to Northwind database successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Northwind database: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Connect to destination database (SDM_project)\n",
    "    try:\n",
    "        print(\"Connecting to SDM_project database...\")\n",
    "        dest_conn = pyodbc.connect(sdm_conn_str)\n",
    "        print(\"Connected to SDM_project database successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to SDM_project database: {str(e)}\")\n",
    "        source_conn.close()\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Disable constraints in destination\n",
    "        tables_with_constraints = disable_constraints(dest_conn)\n",
    "        \n",
    "        # Process base tables first (those without foreign key dependencies)\n",
    "        base_tables = ['Categories', 'Customers', 'Employees', 'Shippers', 'Suppliers', 'Region']\n",
    "        dependent_tables = ['Products', 'Orders', 'Territories', '[Order Details]', 'EmployeeTerritories']\n",
    "        \n",
    "        # Process base tables first\n",
    "        print(\"\\nProcessing base tables...\")\n",
    "        for source_table in base_tables:\n",
    "            if source_table in tables_to_transfer:\n",
    "                dest_table = tables_to_transfer[source_table]\n",
    "                copy_table_data(source_conn, dest_conn, source_table, dest_table)\n",
    "        \n",
    "        # Process dependent tables\n",
    "        print(\"\\nProcessing dependent tables...\")\n",
    "        for source_table in dependent_tables:\n",
    "            if source_table in tables_to_transfer:\n",
    "                dest_table = tables_to_transfer[source_table]\n",
    "                copy_table_data(source_conn, dest_conn, source_table, dest_table)\n",
    "        \n",
    "        # Re-enable constraints\n",
    "        enable_constraints(dest_conn, tables_with_constraints)\n",
    "        \n",
    "        print(\"Data transfer completed successfully\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during data transfer: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Close connections\n",
    "        source_conn.close()\n",
    "        dest_conn.close()\n",
    "        print(\"Database connections closed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDM-> DWH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-09 20:23:50.995823] Connected to both databases on LAPTOP-HLMO0COA\\SQLEXPRESS\n",
      "[2025-04-09 20:23:51.017168] Found 30 tables to process\n",
      "[2025-04-09 20:23:51.091435] Disabled all constraints\n",
      "[2025-04-09 20:23:51.091567] Processing table 1/30: Address\n",
      "[2025-04-09 20:23:58.644977]   - Inserted 10000 rows into Address\n",
      "[2025-04-09 20:24:04.096525] Completed Address: 19614 rows transferred\n",
      "[2025-04-09 20:24:04.096810] Processing table 2/30: billOfMaterials\n",
      "[2025-04-09 20:24:05.612267] Completed billOfMaterials: 2679 rows transferred\n",
      "[2025-04-09 20:24:05.612551] Processing table 3/30: Bonus\n",
      "[2025-04-09 20:24:05.640143] Completed Bonus: 37 rows transferred\n",
      "[2025-04-09 20:24:05.640227] Processing table 4/30: Categories\n",
      "[2025-04-09 20:24:05.669447] Completed Categories: 8 rows transferred\n",
      "[2025-04-09 20:24:05.669514] Processing table 5/30: Customer\n",
      "[2025-04-09 20:24:12.691785]   - Inserted 10000 rows into Customer\n",
      "[2025-04-09 20:24:19.217311]   - Inserted 20000 rows into Customer\n",
      "[2025-04-09 20:24:19.351124] Completed Customer: 20037 rows transferred\n",
      "[2025-04-09 20:24:19.351220] Processing table 6/30: CustomerCustomerDemo\n",
      "[2025-04-09 20:24:19.356185] Table CustomerCustomerDemo is empty, skipping\n",
      "[2025-04-09 20:24:19.356242] Processing table 7/30: CustomerDemographics\n",
      "[2025-04-09 20:24:19.357873] Table CustomerDemographics is empty, skipping\n",
      "[2025-04-09 20:24:19.357913] Processing table 8/30: Department\n",
      "[2025-04-09 20:24:19.370199] Completed Department: 5 rows transferred\n",
      "[2025-04-09 20:24:19.370288] Processing table 9/30: Employee\n",
      "[2025-04-09 20:24:20.237705] Completed Employee: 374 rows transferred\n",
      "[2025-04-09 20:24:20.237980] Processing table 10/30: EmployeeTerritories\n",
      "[2025-04-09 20:24:20.266517] Completed EmployeeTerritories: 49 rows transferred\n",
      "[2025-04-09 20:24:20.266581] Processing table 11/30: OrderDetails\n",
      "[2025-04-09 20:24:21.350742] Completed OrderDetails: 2155 rows transferred\n",
      "[2025-04-09 20:24:21.351290] Processing table 12/30: Orders\n",
      "[2025-04-09 20:24:22.126679] Completed Orders: 830 rows transferred\n",
      "[2025-04-09 20:24:22.127004] Processing table 13/30: person\n",
      "[2025-04-09 20:24:28.764353]   - Inserted 10000 rows into person\n",
      "[2025-04-09 20:24:34.187750] Completed person: 19972 rows transferred\n",
      "[2025-04-09 20:24:34.188010] Processing table 14/30: Product\n",
      "[2025-04-09 20:24:35.295365] Completed Product: 591 rows transferred\n",
      "[2025-04-09 20:24:35.295654] Processing table 15/30: productCategory\n",
      "[2025-04-09 20:24:35.307269] Completed productCategory: 4 rows transferred\n",
      "[2025-04-09 20:24:35.307339] Processing table 16/30: purchaseOrderDetail\n",
      "[2025-04-09 20:24:40.430835] Completed purchaseOrderDetail: 8845 rows transferred\n",
      "[2025-04-09 20:24:40.431092] Processing table 17/30: purchaseOrderHeader\n",
      "[2025-04-09 20:24:42.914462] Completed purchaseOrderHeader: 4012 rows transferred\n",
      "[2025-04-09 20:24:42.914726] Processing table 18/30: purchasing_vendor\n",
      "[2025-04-09 20:24:43.048637] Completed purchasing_vendor: 104 rows transferred\n",
      "[2025-04-09 20:24:43.048715] Processing table 19/30: Region\n",
      "[2025-04-09 20:24:43.055849] Completed Region: 4 rows transferred\n",
      "[2025-04-09 20:24:43.055918] Processing table 20/30: resources_department\n",
      "[2025-04-09 20:24:43.089350] Completed resources_department: 16 rows transferred\n",
      "[2025-04-09 20:24:43.089419] Processing table 21/30: Sales_Order\n",
      "[2025-04-09 20:24:43.488852] Completed Sales_Order: 650 rows transferred\n",
      "[2025-04-09 20:24:43.489098] Processing table 22/30: sales_order_item\n",
      "[2025-04-09 20:24:44.053947] Completed sales_order_item: 1103 rows transferred\n",
      "[2025-04-09 20:24:44.054535] Processing table 23/30: Sales_OrderDetail\n",
      "[2025-04-09 20:24:51.783179]   - Inserted 10000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:24:57.551092]   - Inserted 20000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:03.379056]   - Inserted 30000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:09.127742]   - Inserted 40000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:14.802359]   - Inserted 50000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:20.260166]   - Inserted 60000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:26.001879]   - Inserted 70000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:31.491666]   - Inserted 80000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:36.886198]   - Inserted 90000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:41.951310]   - Inserted 100000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:47.163323]   - Inserted 110000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:52.445743]   - Inserted 120000 rows into Sales_OrderDetail\n",
      "[2025-04-09 20:25:53.181345] Completed Sales_OrderDetail: 121317 rows transferred\n",
      "[2025-04-09 20:25:53.181600] Processing table 24/30: Sales_OrderHeader\n",
      "[2025-04-09 20:26:01.064347]   - Inserted 10000 rows into Sales_OrderHeader\n",
      "[2025-04-09 20:26:07.490886]   - Inserted 20000 rows into Sales_OrderHeader\n",
      "[2025-04-09 20:26:14.057090]   - Inserted 30000 rows into Sales_OrderHeader\n",
      "[2025-04-09 20:26:14.998245] Completed Sales_OrderHeader: 31465 rows transferred\n",
      "[2025-04-09 20:26:14.998489] Processing table 25/30: Sales_Store\n",
      "[2025-04-09 20:26:15.405166] Completed Sales_Store: 701 rows transferred\n",
      "[2025-04-09 20:26:15.405784] Processing table 26/30: Sales_territory\n",
      "[2025-04-09 20:26:15.429905] Completed Sales_territory: 10 rows transferred\n",
      "[2025-04-09 20:26:15.429988] Processing table 27/30: Shippers\n",
      "[2025-04-09 20:26:15.437202] Completed Shippers: 3 rows transferred\n",
      "[2025-04-09 20:26:15.437259] Processing table 28/30: State\n",
      "[2025-04-09 20:26:15.515607] Completed State: 63 rows transferred\n",
      "[2025-04-09 20:26:15.515714] Processing table 29/30: Suppliers\n",
      "[2025-04-09 20:26:15.592776] Completed Suppliers: 29 rows transferred\n",
      "[2025-04-09 20:26:15.592877] Processing table 30/30: Territories\n",
      "[2025-04-09 20:26:15.621730] Completed Territories: 53 rows transferred\n",
      "[2025-04-09 20:26:15.621878] Processing Customer table with demographics\n",
      "[2025-04-09 20:26:28.890889] Completed Customer table: 20037 rows transferred with demographics\n",
      "[2025-04-09 20:26:28.919808] Error re-enabling constraints: ('23000', '[23000] [Microsoft][ODBC SQL Server Driver][SQL Server]The ALTER TABLE statement conflicted with the FOREIGN KEY constraint \"FK__Bonus__emp_id__6B24EA82\". The conflict occurred in database \"DWH_project\", table \"dbo.Employee\", column \\'id\\'. (547) (SQLExecDirectW)')\n",
      "[2025-04-09 20:26:28.921620] ETL process completed\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import datetime\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Print a timestamped log message\"\"\"\n",
    "    print(f\"[{datetime.datetime.now()}] {message}\")\n",
    "\n",
    "def main():\n",
    "    # Connection info\n",
    "    server = 'LAPTOP-HLMO0COA\\\\SQLEXPRESS'\n",
    "    sdm_db = 'SDM_project'\n",
    "    dwh_db = 'DWH_project'\n",
    "    \n",
    "    # Connect to both databases\n",
    "    conn_string = f'DRIVER={{SQL Server}};SERVER={server};Trusted_Connection=yes;'\n",
    "    sdm_conn = pyodbc.connect(conn_string + f'DATABASE={sdm_db}')\n",
    "    dwh_conn = pyodbc.connect(conn_string + f'DATABASE={dwh_db}')\n",
    "    \n",
    "    log(f\"Connected to both databases on {server}\")\n",
    "    \n",
    "    # Get list of tables from SDM\n",
    "    sdm_cursor = sdm_conn.cursor()\n",
    "    sdm_cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n",
    "    tables = [row.TABLE_NAME for row in sdm_cursor.fetchall()]\n",
    "    \n",
    "    log(f\"Found {len(tables)} tables to process\")\n",
    "    \n",
    "    # Disable constraints in DWH\n",
    "    dwh_cursor = dwh_conn.cursor()\n",
    "    try:\n",
    "        dwh_cursor.execute(\"EXEC sp_msforeachtable 'ALTER TABLE ? NOCHECK CONSTRAINT all'\")\n",
    "        dwh_conn.commit()\n",
    "        log(\"Disabled all constraints\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error disabling constraints: {str(e)}\")\n",
    "    \n",
    "    # Process each table\n",
    "    for i, table in enumerate(tables):\n",
    "        try:\n",
    "            log(f\"Processing table {i+1}/{len(tables)}: {table}\")\n",
    "            \n",
    "            # Get data from SDM\n",
    "            sdm_cursor.execute(f\"SELECT * FROM [{table}]\")\n",
    "            rows = sdm_cursor.fetchall()\n",
    "            \n",
    "            if not rows:\n",
    "                log(f\"Table {table} is empty, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Get column names\n",
    "            columns = [column[0] for column in sdm_cursor.description]\n",
    "            \n",
    "            # Clear existing data in DWH\n",
    "            dwh_cursor.execute(f\"DELETE FROM [{table}]\")\n",
    "            dwh_conn.commit()\n",
    "            \n",
    "            # Prepare insert statement\n",
    "            placeholders = ','.join(['?' for _ in columns])\n",
    "            column_names = ','.join([f\"[{col}]\" for col in columns])\n",
    "            insert_sql = f\"INSERT INTO [{table}] ({column_names}) VALUES ({placeholders})\"\n",
    "            \n",
    "            # Insert data into DWH\n",
    "            counter = 0\n",
    "            batch_size = 1000\n",
    "            \n",
    "            for batch_start in range(0, len(rows), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(rows))\n",
    "                batch = rows[batch_start:batch_end]\n",
    "                \n",
    "                dwh_cursor.executemany(insert_sql, batch)\n",
    "                dwh_conn.commit()\n",
    "                \n",
    "                counter += len(batch)\n",
    "                if counter % 10000 == 0:\n",
    "                    log(f\"  - Inserted {counter} rows into {table}\")\n",
    "            \n",
    "            log(f\"Completed {table}: {counter} rows transferred\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log(f\"Error processing table {table}: {str(e)}\")\n",
    "    \n",
    "    # Special handling for Customer table - merge with CustomerDemographics\n",
    "    try:\n",
    "        log(\"Processing Customer table with demographics\")\n",
    "        \n",
    "        # Clear existing data\n",
    "        dwh_cursor.execute(\"DELETE FROM Customer\")\n",
    "        dwh_conn.commit()\n",
    "        \n",
    "        # Create a query that joins the customer tables\n",
    "        merge_query = \"\"\"\n",
    "        SELECT \n",
    "            c.*,\n",
    "            cd.CustomerTypeID,\n",
    "            cd.CustomerDesc\n",
    "        FROM \n",
    "            Customer c\n",
    "        LEFT JOIN \n",
    "            CustomerCustomerDemo ccd ON c.CustomerId = ccd.CustomerID\n",
    "        LEFT JOIN \n",
    "            CustomerDemographics cd ON ccd.CustomerTypeID = cd.CustomerTypeID\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get merged data\n",
    "        sdm_cursor.execute(merge_query)\n",
    "        rows = sdm_cursor.fetchall()\n",
    "        \n",
    "        if rows:\n",
    "            # Get column names\n",
    "            columns = [column[0] for column in sdm_cursor.description]\n",
    "            \n",
    "            # Prepare insert statement\n",
    "            placeholders = ','.join(['?' for _ in columns])\n",
    "            column_names = ','.join([f\"[{col}]\" for col in columns])\n",
    "            insert_sql = f\"INSERT INTO Customer ({column_names}) VALUES ({placeholders})\"\n",
    "            \n",
    "            # Insert data\n",
    "            dwh_cursor.executemany(insert_sql, rows)\n",
    "            dwh_conn.commit()\n",
    "            \n",
    "            log(f\"Completed Customer table: {len(rows)} rows transferred with demographics\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error processing Customer table: {str(e)}\")\n",
    "    \n",
    "    # Re-enable constraints\n",
    "    try:\n",
    "        dwh_cursor.execute(\"EXEC sp_msforeachtable 'ALTER TABLE ? WITH CHECK CHECK CONSTRAINT all'\")\n",
    "        dwh_conn.commit()\n",
    "        log(\"Re-enabled all constraints\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error re-enabling constraints: {str(e)}\")\n",
    "    \n",
    "    # Close connections\n",
    "    sdm_conn.close()\n",
    "    dwh_conn.close()\n",
    "    log(\"ETL process completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
