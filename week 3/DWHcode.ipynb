{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty de tabellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Disabling constraints for country...\n",
      "  Disabling constraints for order_details...\n",
      "  Disabling constraints for order_header...\n",
      "  Disabling constraints for product...\n",
      "  Disabling constraints for product_type...\n",
      "  Disabling constraints for retailer...\n",
      "  Disabling constraints for retailer_contact...\n",
      "  Disabling constraints for retailer_headquarters...\n",
      "  Disabling constraints for retailer_site...\n",
      "  Disabling constraints for returned_item...\n",
      "  Disabling constraints for sales_branch...\n",
      "  Disabling constraints for sales_demographic...\n",
      "  Disabling constraints for sales_staff...\n",
      "  Disabling constraints for training...\n",
      "All constraints disabled\n",
      " Legen van tabel: age_group\n",
      " Legen van tabel: country\n",
      " Legen van tabel: course\n",
      " Legen van tabel: order_details\n",
      " Legen van tabel: order_header\n",
      " Legen van tabel: order_method\n",
      " Legen van tabel: product\n",
      " Legen van tabel: product_line\n",
      " Legen van tabel: product_type\n",
      " Legen van tabel: retailer\n",
      " Legen van tabel: retailer_contact\n",
      " Legen van tabel: retailer_headquarters\n",
      " Legen van tabel: retailer_segment\n",
      " Legen van tabel: retailer_site\n",
      " Legen van tabel: retailer_type\n",
      " Legen van tabel: return_reason\n",
      " Legen van tabel: returned_item\n",
      " Legen van tabel: sales_branch\n",
      " Legen van tabel: sales_demographic\n",
      " Legen van tabel: sales_staff\n",
      " Legen van tabel: sales_territory\n",
      " Legen van tabel: satisfaction_type\n",
      " Legen van tabel: training\n",
      "  Re-enabling constraints for country...\n",
      "  Re-enabling constraints for order_details...\n",
      "  Re-enabling constraints for order_header...\n",
      "  Re-enabling constraints for product...\n",
      "  Re-enabling constraints for product_type...\n",
      "  Re-enabling constraints for retailer...\n",
      "  Re-enabling constraints for retailer_contact...\n",
      "  Re-enabling constraints for retailer_headquarters...\n",
      "  Re-enabling constraints for retailer_site...\n",
      "  Re-enabling constraints for returned_item...\n",
      "  Re-enabling constraints for sales_branch...\n",
      "  Re-enabling constraints for sales_demographic...\n",
      "  Re-enabling constraints for sales_staff...\n",
      "  Re-enabling constraints for training...\n",
      "All constraints re-enabled successfully\n",
      " Alle tabellen in de database 'sdm' zijn geleegd.\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    'SERVER=MSI\\\\SQLEXPRESS;' \n",
    "    'DATABASE=sdm;' \n",
    "    'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "tables_query = \"\"\"\n",
    "SELECT DISTINCT OBJECT_NAME(parent_object_id) AS TableName\n",
    "FROM sys.foreign_keys\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(tables_query)\n",
    "    tables_with_constraints = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    for table in tables_with_constraints:\n",
    "        print(f\"  Disabling constraints for {table}...\")\n",
    "        cursor.execute(f\"ALTER TABLE [{table}] NOCHECK CONSTRAINT ALL\")\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"All constraints disabled\")\n",
    "except Exception as e:\n",
    "    print(f\"Error disabling constraints: {str(e)}\")\n",
    "    # Continue anyway - we'll still try to import data\n",
    "\n",
    "\n",
    "cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n",
    "all_tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "\n",
    "for table_name in all_tables:\n",
    "    print(f\" Legen van tabel: {table_name}\")\n",
    "    try:\n",
    "        cursor.execute(f\"DELETE FROM [{table_name}];\")\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting from {table_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Re-enable constraints for each table individually\n",
    "    for table in tables_with_constraints:\n",
    "        print(f\"  Re-enabling constraints for {table}...\")\n",
    "        cursor.execute(f\"ALTER TABLE [{table}] WITH CHECK CHECK CONSTRAINT ALL\")\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"All constraints re-enabled successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error re-enabling constraints: {str(e)}\")\n",
    "\n",
    "print(\" Alle tabellen in de database 'sdm' zijn geleegd.\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin import van data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling constraints...\n",
      "  Disabling constraints for COUNTRY...\n",
      "  Disabling constraints for INVENTORY_LEVELS...\n",
      "  Disabling constraints for ORDER_DETAILS...\n",
      "  Disabling constraints for ORDER_HEADER...\n",
      "  Disabling constraints for PRODUCT...\n",
      "  Disabling constraints for PRODUCT_FORECAST...\n",
      "  Disabling constraints for PRODUCT_TYPE...\n",
      "  Disabling constraints for RETAILER...\n",
      "  Disabling constraints for RETAILER_CONTACT...\n",
      "  Disabling constraints for RETAILER_HEADQUARTERS...\n",
      "  Disabling constraints for RETAILER_SITE...\n",
      "  Disabling constraints for RETURNED_ITEM...\n",
      "  Disabling constraints for SALES_BRANCH...\n",
      "  Disabling constraints for SALES_DEMOGRAPHIC...\n",
      "  Disabling constraints for SALES_STAFF...\n",
      "  Disabling constraints for SATISFACTION...\n",
      "  Disabling constraints for TRAINING...\n",
      "All constraints disabled\n",
      "Importing from sales database...\n",
      "  Skipping country -> COUNTRY (will be merged later)\n",
      "  Importing order_details -> ORDER_DETAILS\n",
      "    Successfully imported 37757 rows\n",
      "  Importing order_header -> ORDER_HEADER\n",
      "    Successfully imported 4784 rows\n",
      "  Importing order_method -> ORDER_METHOD\n",
      "    Successfully imported 7 rows\n",
      "  Importing product_line -> PRODUCT_LINE\n",
      "    Successfully imported 5 rows\n",
      "  Importing product_type -> PRODUCT_TYPE\n",
      "    Successfully imported 21 rows\n",
      "  Importing product -> PRODUCT\n",
      "    Successfully imported 115 rows\n",
      "  Importing return_reason -> RETURN_REASON\n",
      "    Successfully imported 5 rows\n",
      "  Importing returned_item -> RETURNED_ITEM\n",
      "    Successfully imported 619 rows\n",
      "  Skipping sales_branch -> SALES_BRANCH (will be merged later)\n",
      "  Skipping sales_staff -> SALES_STAFF (will be merged later)\n",
      "  Skipping retailer_site -> RETAILER_SITE (will be merged later)\n",
      "Importing from crm database...\n",
      "  Importing age_group -> AGE_GROUP\n",
      "    Successfully imported 6 rows\n",
      "  Skipping country -> COUNTRY (will be merged later)\n",
      "  Importing retailer_segment -> RETAILER_SEGMENT\n",
      "    Successfully imported 12 rows\n",
      "  Importing sales_territory -> SALES_TERRITORY\n",
      "    Successfully imported 5 rows\n",
      "  Importing retailer -> RETAILER\n",
      "    Successfully imported 109 rows\n",
      "  Skipping retailer_site -> RETAILER_SITE (will be merged later)\n",
      "  Importing retailer_type -> RETAILER_TYPE\n",
      "    Successfully imported 8 rows\n",
      "  Importing sales_demographic -> SALES_DEMOGRAPHIC\n",
      "    Successfully imported 2484 rows\n",
      "  Importing retailer_headquarters -> RETAILER_HEADQUARTERS\n",
      "    Successfully imported 414 rows\n",
      "  Importing retailer_contact -> RETAILER_CONTACT\n",
      "    Successfully imported 391 rows\n",
      "Importing from staff database...\n",
      "  Importing course -> COURSE\n",
      "    Successfully imported 9 rows\n",
      "  Skipping sales_branch -> SALES_BRANCH (will be merged later)\n",
      "  Skipping sales_staff -> SALES_STAFF (will be merged later)\n",
      "  Importing satisfaction -> SATISFACTION\n",
      "    Successfully imported 202 rows\n",
      "  Importing satisfaction_type -> SATISFACTION_TYPE\n",
      "    Successfully imported 5 rows\n",
      "  Importing training -> TRAINING\n",
      "    Successfully imported 319 rows\n",
      "Importing merged table COUNTRY...\n",
      "  Retrieved 21 rows from sales.country\n",
      "  Retrieved 21 rows from crm.country\n",
      "  After deduplication: 21 rows\n",
      "  Performing special merge for COUNTRY table using pd.merge...\n",
      "  After COUNTRY pd.merge: 21 rows\n",
      "  Successfully imported 21 rows\n",
      "Importing merged table SALES_BRANCH...\n",
      "  Error retrieving data from sales.sales_branch: Execution failed on sql 'SELECT SALES_BRANCH_CODE, ADDRESS1, ADDRESS2, CITY, REGION, POSTAL_ZONE, COUNTRY_CODE FROM sales_branch': no such column: POSTAL_ZONE\n",
      "  Error retrieving data from staff.sales_branch: Execution failed on sql 'SELECT SALES_BRANCH_CODE, ADDRESS1, ADDRESS2, CITY, REGION, POSTAL_ZONE, COUNTRY_CODE FROM sales_branch': no such column: POSTAL_ZONE\n",
      "  No data retrieved for SALES_BRANCH. Skipping.\n",
      "Importing merged table SALES_STAFF...\n",
      "  Retrieved 102 rows from sales.sales_staff\n",
      "  Retrieved 102 rows from staff.sales_staff\n",
      "  After deduplication: 102 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_32000\\3826899253.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_32000\\3826899253.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].where(pd.notnull(df[col]), None)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_32000\\3826899253.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_32000\\3826899253.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].where(pd.notnull(df[col]), None)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_32000\\3826899253.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(int)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_32000\\3826899253.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully imported 102 rows\n",
      "Importing merged table RETAILER_SITE...\n",
      "  Error retrieving data from sales.retailer_site: Execution failed on sql 'SELECT RETAILER_SITE_CODE, RETAILER_CODE, ADDRESS1, ADDRESS2, CITY, REGION, POSTAL_ZONE, COUNTRY_CODE, ACTIVE_INDICATOR FROM retailer_site': no such column: POSTAL_ZONE\n",
      "  Error retrieving data from crm.retailer_site: Execution failed on sql 'SELECT RETAILER_SITE_CODE, RETAILER_CODE, ADDRESS1, ADDRESS2, CITY, REGION, POSTAL_ZONE, COUNTRY_CODE, ACTIVE_INDICATOR FROM retailer_site': no such column: POSTAL_ZONE\n",
      "  No data retrieved for RETAILER_SITE. Skipping.\n",
      "  Importing inventory_levels_train.csv -> INVENTORY_LEVELS\n",
      "    Successfully imported 3543 rows\n",
      "  Importing product_forecast_train.csv -> PRODUCT_FORECAST\n",
      "    Successfully imported 3529 rows\n",
      "Re-enabling constraints...\n",
      "  Re-enabling constraints for COUNTRY...\n",
      "  Re-enabling constraints for INVENTORY_LEVELS...\n",
      "  Re-enabling constraints for ORDER_DETAILS...\n",
      "  Re-enabling constraints for ORDER_HEADER...\n",
      "Error re-enabling constraints: ('23000', '[23000] [Microsoft][ODBC SQL Server Driver][SQL Server]The ALTER TABLE statement conflicted with the FOREIGN KEY constraint \"FK__ORDER_HEA__SALES__6C190EBB\". The conflict occurred in database \"sdm\", table \"dbo.SALES_BRANCH\", column \\'SALES_BRANCH_CODE\\'. (547) (SQLExecDirectW)')\n",
      "SQL Server connection closed\n",
      "Import completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pyodbc\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Database paths\n",
    "db_paths = {\n",
    "    'sales': 'go_sales_train.sqlite',\n",
    "    'crm': 'go_crm_train.sqlite',\n",
    "    'staff': 'go_staff_train.sqlite'\n",
    "}\n",
    "\n",
    "csv_files = {\n",
    "    \"INVENTORY_LEVELS\": 'inventory_levels_train.csv',\n",
    "    \"PRODUCT_FORECAST\": 'product_forecast_train.csv',\n",
    "}\n",
    "\n",
    "# Direct SQL Server connection with pyodbc\n",
    "server = 'localhost\\\\SQLEXPRESS'  # Change to your SQL Server instance\n",
    "database = 'sdm'\n",
    "conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;'\n",
    "sql_conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Tables to merge from different databases\n",
    "# Format: table_name: {db_name: [column_list], 'column_mapping': {source_column: target_column}}\n",
    "merged_tables = {\n",
    "    'COUNTRY': {\n",
    "        'sales': ['COUNTRY_CODE', 'COUNTRY', 'LANGUAGE', 'CURRENCY_NAME'],\n",
    "        'crm': ['COUNTRY_CODE', 'COUNTRY_EN', 'FLAG_IMAGE', 'SALES_TERRITORY_CODE'],\n",
    "\n",
    "    },\n",
    "    'SALES_BRANCH': {\n",
    "        'sales': ['SALES_BRANCH_CODE', 'ADDRESS1', 'ADDRESS2', 'CITY', 'REGION', 'POSTAL_ZONE', 'COUNTRY_CODE'],\n",
    "        'staff': ['SALES_BRANCH_CODE', 'ADDRESS1', 'ADDRESS2', 'CITY', 'REGION', 'POSTAL_ZONE', 'COUNTRY_CODE']\n",
    "    },\n",
    "    'SALES_STAFF': {\n",
    "        'sales': ['SALES_STAFF_CODE', 'FIRST_NAME', 'LAST_NAME', 'POSITION_EN', 'WORK_PHONE', 'EXTENSION', 'FAX', 'EMAIL', 'DATE_HIRED', 'SALES_BRANCH_CODE'],\n",
    "        'staff': ['SALES_STAFF_CODE', 'FIRST_NAME', 'LAST_NAME', 'POSITION_EN', 'WORK_PHONE', 'EXTENSION', 'FAX', 'EMAIL', 'DATE_HIRED', 'SALES_BRANCH_CODE', 'MANAGER_CODE']\n",
    "    },\n",
    "    'RETAILER_SITE': {\n",
    "        'sales': ['RETAILER_SITE_CODE', 'RETAILER_CODE', 'ADDRESS1', 'ADDRESS2', 'CITY', 'REGION', 'POSTAL_ZONE', 'COUNTRY_CODE', 'ACTIVE_INDICATOR'],\n",
    "        'crm': ['RETAILER_SITE_CODE', 'RETAILER_CODE', 'ADDRESS1', 'ADDRESS2', 'CITY', 'REGION', 'POSTAL_ZONE', 'COUNTRY_CODE', 'ACTIVE_INDICATOR']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Disable constraints - use a more direct approach that doesn't depend on sp_MSforeachtable\n",
    "cursor = sql_conn.cursor()\n",
    "print(\"Disabling constraints...\")\n",
    "\n",
    "# Get all tables with constraints\n",
    "tables_query = \"\"\"\n",
    "SELECT DISTINCT OBJECT_NAME(parent_object_id) AS TableName\n",
    "FROM sys.foreign_keys\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(tables_query)\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    # Disable constraints for each table individually\n",
    "    for table in tables:\n",
    "        print(f\"  Disabling constraints for {table}...\")\n",
    "        cursor.execute(f\"ALTER TABLE [{table}] NOCHECK CONSTRAINT ALL\")\n",
    "        sql_conn.commit()\n",
    "    \n",
    "    print(\"All constraints disabled\")\n",
    "except Exception as e:\n",
    "    print(f\"Error disabling constraints: {str(e)}\")\n",
    "    # Continue anyway - we'll still try to import data\n",
    "\n",
    "def handle_numpy_types(df):\n",
    "    \"\"\"Convert numpy data types to Python native types for pyodbc compatibility\"\"\"\n",
    "    for col in df.columns:\n",
    "        # Convert numpy.int64/int32 to Python int\n",
    "        if df[col].dtype == np.int64 or df[col].dtype == np.int32:\n",
    "            df[col] = df[col].astype(int)\n",
    "        # Convert numpy.float64/float32 to Python float\n",
    "        elif df[col].dtype == np.float64 or df[col].dtype == np.float32:\n",
    "            df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "def get_sql_column_types(sql_table):\n",
    "    \"\"\"Get column data types from SQL Server\"\"\"\n",
    "    cursor = sql_conn.cursor()\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME, DATA_TYPE\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_NAME = '{sql_table}'\n",
    "    \"\"\")\n",
    "    return {row[0]: row[1] for row in cursor.fetchall()}\n",
    "\n",
    "def fix_data_types(df, sql_table):\n",
    "    \"\"\"Fix data types based on SQL Server column types\"\"\"\n",
    "    sql_types = get_sql_column_types(sql_table)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in sql_types:\n",
    "            # Handle numeric columns\n",
    "            if sql_types[col] == 'float' or sql_types[col] == 'real':\n",
    "                # Convert to float and replace NaN with None\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                df[col] = df[col].astype(float)\n",
    "            \n",
    "            # Handle integer columns\n",
    "            elif sql_types[col] in ('int', 'smallint', 'tinyint', 'bigint'):\n",
    "                # First convert to float in case there are NaN values, then to int\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                # Replace NaN with None/NULL\n",
    "                df[col] = df[col].where(pd.notnull(df[col]), None)\n",
    "            \n",
    "            # Handle date columns\n",
    "            elif sql_types[col] in ('date', 'datetime', 'datetime2'):\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_primary_key_columns(sql_table):\n",
    "    \"\"\"Get primary key columns for a SQL table\"\"\"\n",
    "    cursor = sql_conn.cursor()\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME\n",
    "        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
    "        WHERE OBJECTPROPERTY(OBJECT_ID(CONSTRAINT_SCHEMA + '.' + CONSTRAINT_NAME), 'IsPrimaryKey') = 1\n",
    "        AND TABLE_NAME = '{sql_table}'\n",
    "    \"\"\")\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def check_for_duplicates(df, sql_table):\n",
    "    \"\"\"Check if primary key values already exist in SQL Server\"\"\"\n",
    "    pk_columns = get_primary_key_columns(sql_table)\n",
    "    \n",
    "    if not pk_columns:\n",
    "        return df  # No primary key, no need to check\n",
    "    \n",
    "    # Check if all primary key columns exist in the dataframe\n",
    "    if not all(col in df.columns for col in pk_columns):\n",
    "        print(f\"    Warning: Not all primary key columns exist in the dataframe. Skipping duplicate check.\")\n",
    "        return df\n",
    "    \n",
    "    # Create a list to keep track of duplicate rows\n",
    "    duplicate_indices = []\n",
    "    cursor = sql_conn.cursor()\n",
    "    \n",
    "    # Generate WHERE clause to check for existing keys\n",
    "    for i in range(len(df)):\n",
    "        where_clauses = []\n",
    "        for col in pk_columns:\n",
    "            value = df.iloc[i][col]\n",
    "            \n",
    "            if pd.isna(value):\n",
    "                where_clause = f\"{col} IS NULL\"\n",
    "            elif isinstance(value, str):\n",
    "                where_clause = f\"{col} = '{value.replace(\"'\", \"''\")}'\"\n",
    "            else:\n",
    "                where_clause = f\"{col} = {value}\"\n",
    "                \n",
    "            where_clauses.append(where_clause)\n",
    "        \n",
    "        where_str = \" AND \".join(where_clauses)\n",
    "        query = f\"SELECT 1 FROM {sql_table} WHERE {where_str}\"\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        if cursor.fetchone():\n",
    "            # Add this index to our list of duplicates\n",
    "            duplicate_indices.append(i)\n",
    "    \n",
    "    # If we found duplicates, remove them\n",
    "    if duplicate_indices:\n",
    "        print(f\"    Skipping {len(duplicate_indices)} duplicate rows\")\n",
    "        # Create a new dataframe without the duplicate rows\n",
    "        df = df.drop(duplicate_indices)\n",
    "        # Reset the index to avoid issues with missing indices\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def import_merged_table(table_name):\n",
    "    \"\"\"Import and merge a table from multiple SQLite databases\"\"\"\n",
    "    print(f\"Importing merged table {table_name}...\")\n",
    "    \n",
    "    all_dfs = []\n",
    "    db_connections = {}\n",
    "    \n",
    "    try:\n",
    "        # Get SQL Server column info\n",
    "        cursor = sql_conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} WHERE 1=0\")\n",
    "        target_columns = [column[0] for column in cursor.description]\n",
    "        \n",
    "        # Get column mapping if it exists\n",
    "        column_mapping = {}\n",
    "        if 'column_mapping' in merged_tables[table_name]:\n",
    "            column_mapping = merged_tables[table_name]['column_mapping']\n",
    "        \n",
    "        # Connect to each database and load data\n",
    "        for db_name, columns in merged_tables[table_name].items():\n",
    "            # Skip the column_mapping entry\n",
    "            if db_name == 'column_mapping':\n",
    "                continue\n",
    "                \n",
    "            if db_name not in db_connections:\n",
    "                db_path = db_paths[db_name]\n",
    "                db_connections[db_name] = sqlite3.connect(db_path)\n",
    "            \n",
    "            sqlite_conn = db_connections[db_name]\n",
    "            \n",
    "            # Get data from SQLite\n",
    "            sqlite_table = table_name.lower()  # SQLite table names are lowercase\n",
    "            query = f\"SELECT {', '.join(columns)} FROM {sqlite_table}\"\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_sql_query(query, sqlite_conn)\n",
    "                \n",
    "                if not df.empty:\n",
    "                    # Convert column names to uppercase\n",
    "                    df.columns = [col.upper() for col in df.columns]\n",
    "                    \n",
    "                    # Apply column mapping\n",
    "                    for source_col, target_col in column_mapping.items():\n",
    "                        if source_col in df.columns:\n",
    "                            df = df.rename(columns={source_col: target_col})\n",
    "                    \n",
    "                    # Add identifying column to track source\n",
    "                    df['_SOURCE_DB'] = db_name\n",
    "                    \n",
    "                    all_dfs.append(df)\n",
    "                    print(f\"  Retrieved {len(df)} rows from {db_name}.{sqlite_table}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error retrieving data from {db_name}.{sqlite_table}: {str(e)}\")\n",
    "        \n",
    "        # If no data was retrieved, exit\n",
    "        if not all_dfs:\n",
    "            print(f\"  No data retrieved for {table_name}. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # Combine dataframes\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        # Get primary key column(s)\n",
    "        pk_columns = get_primary_key_columns(table_name)\n",
    "        \n",
    "        # If we have a primary key, deduplicate based on it\n",
    "        if pk_columns and all(col in combined_df.columns for col in pk_columns):\n",
    "            # Keep the last occurrence of each primary key (assuming later DBs have priority)\n",
    "            combined_df = combined_df.drop_duplicates(subset=pk_columns, keep='last')\n",
    "            print(f\"  After deduplication: {len(combined_df)} rows\")\n",
    "        \n",
    "        # For the COUNTRY table specifically, use pd.merge to combine data from both sources\n",
    "        if table_name == 'COUNTRY':\n",
    "            print(\"  Performing special merge for COUNTRY table using pd.merge...\")\n",
    "            \n",
    "            # Separate dataframes by source\n",
    "            sales_df = None\n",
    "            crm_df = None\n",
    "            \n",
    "            for df in all_dfs:\n",
    "                if df['_SOURCE_DB'].iloc[0] == 'sales':\n",
    "                    sales_df = df.drop('_SOURCE_DB', axis=1)\n",
    "                elif df['_SOURCE_DB'].iloc[0] == 'crm':\n",
    "                    crm_df = df.drop('_SOURCE_DB', axis=1)\n",
    "            \n",
    "            if sales_df is not None and crm_df is not None:\n",
    "                # Merge dataframes on COUNTRY_CODE\n",
    "                merged_df = pd.merge(\n",
    "                    sales_df, \n",
    "                    crm_df, \n",
    "                    on='COUNTRY_CODE', \n",
    "                    how='outer',\n",
    "                    suffixes=('', '_crm')\n",
    "                )\n",
    "                \n",
    "                # For columns with the same name that got a suffix, choose the non-null value\n",
    "                for col in crm_df.columns:\n",
    "                    if col != 'COUNTRY_CODE' and f'{col}_crm' in merged_df.columns:\n",
    "                        merged_df[col] = merged_df[col].fillna(merged_df[f'{col}_crm'])\n",
    "                        merged_df = merged_df.drop(f'{col}_crm', axis=1)\n",
    "                \n",
    "                combined_df = merged_df\n",
    "                print(f\"  After COUNTRY pd.merge: {len(combined_df)} rows\")\n",
    "            elif sales_df is not None:\n",
    "                combined_df = sales_df\n",
    "                print(f\"  Using only sales data for COUNTRY: {len(combined_df)} rows\")\n",
    "            elif crm_df is not None:\n",
    "                combined_df = crm_df\n",
    "                print(f\"  Using only crm data for COUNTRY: {len(combined_df)} rows\")\n",
    "        \n",
    "        # Drop the source tracking column if it exists\n",
    "        if '_SOURCE_DB' in combined_df.columns:\n",
    "            combined_df = combined_df.drop('_SOURCE_DB', axis=1)\n",
    "        \n",
    "        # Filter to columns that exist in SQL Server\n",
    "        common_columns = [col for col in combined_df.columns if col in target_columns]\n",
    "        if not common_columns:\n",
    "            print(f\"  No matching columns between combined data and {table_name}. Skipping.\")\n",
    "            return\n",
    "            \n",
    "        combined_df = combined_df[common_columns]\n",
    "        \n",
    "        # Fix data types based on SQL Server schema\n",
    "        combined_df = fix_data_types(combined_df, table_name)\n",
    "        \n",
    "        # Check for and remove duplicate primary keys with existing data\n",
    "        combined_df = check_for_duplicates(combined_df, table_name)\n",
    "        \n",
    "        if combined_df.empty:\n",
    "            print(f\"  All rows were duplicates. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # Convert numpy types to Python native types\n",
    "        combined_df = handle_numpy_types(combined_df)\n",
    "        \n",
    "        # Generate placeholders for SQL INSERT\n",
    "        placeholders = ', '.join(['?' for _ in common_columns])\n",
    "        columns_str = ', '.join(common_columns)\n",
    "        \n",
    "        # Prepare insert statement\n",
    "        insert_sql = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})\"\n",
    "        \n",
    "        # Insert data in batches\n",
    "        batch_size = 1000\n",
    "        rows_imported = 0\n",
    "        \n",
    "        for i in range(0, len(combined_df), batch_size):\n",
    "            batch_df = combined_df.iloc[i:i+batch_size]\n",
    "            \n",
    "            # Convert DataFrame to list of tuples\n",
    "            batch_data = []\n",
    "            for _, row in batch_df.iterrows():\n",
    "                # Replace NaN with None for SQL NULL\n",
    "                row_tuple = tuple(None if pd.isna(val) else val for val in row)\n",
    "                batch_data.append(row_tuple)\n",
    "            \n",
    "            cursor.executemany(insert_sql, batch_data)\n",
    "            sql_conn.commit()\n",
    "            rows_imported += len(batch_data)\n",
    "        \n",
    "        print(f\"  Successfully imported {rows_imported} rows\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error importing merged table {table_name}: {str(e)}\")\n",
    "        sql_conn.rollback()\n",
    "    \n",
    "    finally:\n",
    "        # Close all database connections\n",
    "        for conn in db_connections.values():\n",
    "            try:\n",
    "                conn.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def import_table(sqlite_conn, sqlite_table, sql_table, db_name):\n",
    "    \"\"\"Import a table from SQLite to SQL Server using pandas as intermediary\"\"\"\n",
    "    # Skip tables that should be merged\n",
    "    if sql_table in merged_tables and db_name in merged_tables[sql_table]:\n",
    "        print(f\"  Skipping {sqlite_table} -> {sql_table} (will be merged later)\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        print(f\"  Importing {sqlite_table} -> {sql_table}\")\n",
    "        \n",
    "        # Get data from SQLite\n",
    "        query = f\"SELECT * FROM {sqlite_table}\"\n",
    "        df = pd.read_sql_query(query, sqlite_conn)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"    Table {sqlite_table} is empty. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # Convert column names to uppercase\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "        \n",
    "        # Get column info from SQL Server\n",
    "        cursor = sql_conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {sql_table} WHERE 1=0\")\n",
    "        columns = [column[0] for column in cursor.description]\n",
    "        \n",
    "        # Filter dataframe columns to match SQL Server\n",
    "        common_columns = [col for col in df.columns if col in columns]\n",
    "        if not common_columns:\n",
    "            print(f\"    No matching columns between {sqlite_table} and {sql_table}. Skipping.\")\n",
    "            return\n",
    "            \n",
    "        df = df[common_columns]\n",
    "        \n",
    "        # Fix data types based on SQL Server schema\n",
    "        df = fix_data_types(df, sql_table)\n",
    "        \n",
    "        # Check for and remove duplicate primary keys\n",
    "        df = check_for_duplicates(df, sql_table)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"    All rows were duplicates. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # Convert numpy types to Python native types\n",
    "        df = handle_numpy_types(df)\n",
    "        \n",
    "        # Generate placeholders for SQL INSERT\n",
    "        placeholders = ', '.join(['?' for _ in common_columns])\n",
    "        columns_str = ', '.join(common_columns)\n",
    "        \n",
    "        # Prepare insert statement\n",
    "        insert_sql = f\"INSERT INTO {sql_table} ({columns_str}) VALUES ({placeholders})\"\n",
    "        \n",
    "        # Insert data in batches\n",
    "        batch_size = 1000\n",
    "        rows_imported = 0\n",
    "        \n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch_df = df.iloc[i:i+batch_size]\n",
    "            \n",
    "            # Convert DataFrame to list of tuples\n",
    "            batch_data = []\n",
    "            for _, row in batch_df.iterrows():\n",
    "                # Replace NaN with None for SQL NULL\n",
    "                row_tuple = tuple(None if pd.isna(val) else val for val in row)\n",
    "                batch_data.append(row_tuple)\n",
    "            \n",
    "            cursor.executemany(insert_sql, batch_data)\n",
    "            sql_conn.commit()\n",
    "            rows_imported += len(batch_data)\n",
    "            \n",
    "        print(f\"    Successfully imported {rows_imported} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error importing {sqlite_table}: {str(e)}\")\n",
    "        sql_conn.rollback()\n",
    "\n",
    "def import_csv(csv_path, sql_table):\n",
    "    \"\"\"Import a CSV file to SQL Server\"\"\"\n",
    "    try:\n",
    "        print(f\"  Importing {os.path.basename(csv_path)} -> {sql_table}\")\n",
    "        \n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Convert column names to uppercase\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "        \n",
    "        # Handle 'Unnamed: 0' column for inventory_levels\n",
    "        if 'UNNAMED: 0' in df.columns:\n",
    "            df = df.rename(columns={'UNNAMED: 0': 'ID'})\n",
    "        \n",
    "        # Get column info from SQL Server\n",
    "        cursor = sql_conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {sql_table} WHERE 1=0\")\n",
    "        columns = [column[0] for column in cursor.description]\n",
    "        \n",
    "        # Filter dataframe columns to match SQL Server\n",
    "        common_columns = [col for col in df.columns if col in columns]\n",
    "        if not common_columns:\n",
    "            print(f\"    No matching columns between CSV and {sql_table}. Skipping.\")\n",
    "            return\n",
    "            \n",
    "        df = df[common_columns]\n",
    "        \n",
    "        # Fix data types based on SQL Server schema\n",
    "        df = fix_data_types(df, sql_table)\n",
    "        \n",
    "        # Check for and remove duplicate primary keys\n",
    "        df = check_for_duplicates(df, sql_table)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"    All rows were duplicates. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # Convert numpy types to Python native types\n",
    "        df = handle_numpy_types(df)\n",
    "        \n",
    "        # Generate placeholders for SQL INSERT\n",
    "        placeholders = ', '.join(['?' for _ in common_columns])\n",
    "        columns_str = ', '.join(common_columns)\n",
    "        \n",
    "        # Prepare insert statement\n",
    "        insert_sql = f\"INSERT INTO {sql_table} ({columns_str}) VALUES ({placeholders})\"\n",
    "        \n",
    "        # Insert data in batches\n",
    "        batch_size = 1000\n",
    "        rows_imported = 0\n",
    "        \n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch_df = df.iloc[i:i+batch_size]\n",
    "            \n",
    "            # Convert DataFrame to list of tuples, handling NaN values\n",
    "            batch_data = []\n",
    "            for _, row in batch_df.iterrows():\n",
    "                # Replace NaN with None for SQL NULL\n",
    "                row_tuple = tuple(None if pd.isna(val) else val for val in row)\n",
    "                batch_data.append(row_tuple)\n",
    "            \n",
    "            cursor.executemany(insert_sql, batch_data)\n",
    "            sql_conn.commit()\n",
    "            rows_imported += len(batch_data)\n",
    "            \n",
    "        print(f\"    Successfully imported {rows_imported} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error importing {csv_path}: {str(e)}\")\n",
    "        sql_conn.rollback()\n",
    "\n",
    "# Option to truncate tables before import\n",
    "def truncate_table(table_name):\n",
    "    \"\"\"Truncate a table in SQL Server\"\"\"\n",
    "    try:\n",
    "        cursor = sql_conn.cursor()\n",
    "        cursor.execute(f\"DELETE FROM {table_name}\")\n",
    "        sql_conn.commit()\n",
    "        print(f\"  Truncated table {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error truncating {table_name}: {str(e)}\")\n",
    "        sql_conn.rollback()\n",
    "\n",
    "# Set this to True if you want to clear tables before importing\n",
    "TRUNCATE_BEFORE_IMPORT = False  # Change to True if needed\n",
    "\n",
    "# Store SQLite connections to close later\n",
    "sqlite_connections = {}\n",
    "\n",
    "try:\n",
    "    # First, truncate merged tables if needed\n",
    "    if TRUNCATE_BEFORE_IMPORT:\n",
    "        for table_name in merged_tables.keys():\n",
    "            truncate_table(table_name)\n",
    "    \n",
    "    # Process regular (non-merged) tables\n",
    "    for db_name, db_path in db_paths.items():\n",
    "        print(f\"Importing from {db_name} database...\")\n",
    "        \n",
    "        # Connect to SQLite\n",
    "        sqlite_conn = sqlite3.connect(db_path)\n",
    "        sqlite_connections[db_name] = sqlite_conn\n",
    "        \n",
    "        # Get tables\n",
    "        sqlite_cursor = sqlite_conn.cursor()\n",
    "        sqlite_cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\")\n",
    "        tables = [row[0] for row in sqlite_cursor.fetchall()]\n",
    "        \n",
    "        # Import each table\n",
    "        for table in tables:\n",
    "            target_table = table.upper()\n",
    "            \n",
    "            # Optionally truncate table before import (skip for merged tables)\n",
    "            if TRUNCATE_BEFORE_IMPORT and target_table not in merged_tables:\n",
    "                truncate_table(target_table)\n",
    "            \n",
    "            import_table(sqlite_conn, table, target_table, db_name)\n",
    "\n",
    "    # Now process merged tables\n",
    "    for table_name in merged_tables.keys():\n",
    "        import_merged_table(table_name)\n",
    "\n",
    "    # Import CSV files\n",
    "    for table_name, file_path in csv_files.items():\n",
    "        # Optionally truncate table before import\n",
    "        if TRUNCATE_BEFORE_IMPORT:\n",
    "            truncate_table(table_name)\n",
    "            \n",
    "        import_csv(file_path, table_name)\n",
    "\n",
    "    # Re-enable constraints - use a more direct approach\n",
    "    print(\"Re-enabling constraints...\")\n",
    "    sql_cursor = sql_conn.cursor()\n",
    "    \n",
    "    # Get all tables with constraints\n",
    "    tables_query = \"\"\"\n",
    "    SELECT DISTINCT OBJECT_NAME(parent_object_id) AS TableName\n",
    "    FROM sys.foreign_keys\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        sql_cursor.execute(tables_query)\n",
    "        tables = [row[0] for row in sql_cursor.fetchall()]\n",
    "        \n",
    "        # Re-enable constraints for each table individually\n",
    "        for table in tables:\n",
    "            print(f\"  Re-enabling constraints for {table}...\")\n",
    "            sql_cursor.execute(f\"ALTER TABLE [{table}] WITH CHECK CHECK CONSTRAINT ALL\")\n",
    "            sql_conn.commit()\n",
    "        \n",
    "        print(\"All constraints re-enabled successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error re-enabling constraints: {str(e)}\")\n",
    "        # Continue anyway - the import is already complete\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during import process: {str(e)}\")\n",
    "    if 'sql_conn' in locals() and sql_conn:\n",
    "        try:\n",
    "            sql_conn.rollback()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "finally:\n",
    "    # Close all SQLite connections\n",
    "    for conn in sqlite_connections.values():\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    # Ensure SQL Server connection is closed properly\n",
    "    if 'sql_conn' in locals() and sql_conn:\n",
    "        try:\n",
    "            sql_conn.close()\n",
    "            print(\"SQL Server connection closed\")\n",
    "        except:\n",
    "            print(\"Error closing SQL Server connection\")\n",
    "\n",
    "print(\"Import completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
